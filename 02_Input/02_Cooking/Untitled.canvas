{
	"nodes":[
		{"id":"4472ca3d2cb384a9","type":"group","x":-540,"y":3280,"width":9420,"height":9840,"color":"5","label":"Matrix"},
		{"id":"e942e81188bc21ad","type":"group","x":-354,"y":5920,"width":8796,"height":4010,"color":"3","label":"3.5. Subspaces, Basis, Dimension, and Rank"},
		{"id":"b36a16d9921d7dbb","type":"group","x":-334,"y":6660,"width":6680,"height":3250,"color":"6","label":"Subspace"},
		{"id":"6d85521cb91c16b5","type":"group","x":-334,"y":10160,"width":6320,"height":2660,"color":"3","label":"3.6 Intro Linear Transformation"},
		{"id":"43c94965f5225f6b","type":"group","x":-5600,"y":3280,"width":4894,"height":2862,"color":"5","label":"# Vectors"},
		{"id":"45b318a995c93e7c","type":"group","x":-274,"y":10220,"width":5354,"height":2580,"color":"6","label":"Linear Transformations"},
		{"id":"e3f7b2a377736c4d","type":"group","x":3760,"y":3440,"width":4682,"height":2208,"color":"3","label":"3.3 Inverse"},
		{"id":"1e2639c8292dae31","type":"group","x":3426,"y":6820,"width":2820,"height":2100,"color":"6","label":"Basis"},
		{"id":"e229531a0aa0a06d","x":10160,"y":3260,"width":2989,"height":1637,"color":"3","type":"group","label":"## 4.3 Eigenvalues and Eigenvectors of n x n Matrices"},
		{"id":"fb38bf205352f2e2","type":"group","x":-3546,"y":3465,"width":2840,"height":1685,"color":"3","label":"## 1.2 Length and Angle: The Dot Product## 線と角度"},
		{"id":"8137fc4b9b6eabad","type":"group","x":-312,"y":3440,"width":2452,"height":1875,"color":"3","label":"3.1 Matrix Operations"},
		{"id":"8c95a6585ec31b8f","type":"group","x":6386,"y":6660,"width":2036,"height":1740,"color":"6","label":"Dimension and Rank"},
		{"id":"9ae32d649267b42c","type":"group","x":6086,"y":4491,"width":2356,"height":1157,"color":"1","label":"Invertible??"},
		{"id":"22c968707bffca50","type":"group","x":-5600,"y":4900,"width":1953,"height":1242,"color":"3","label":"## 1.3 Lines and Planes"},
		{"id":"c675cb1bcbe584db","type":"group","x":-5486,"y":3465,"width":1725,"height":1215,"color":"3","label":"## 1.1 The Geometry and Algebra of Vectors"},
		{"id":"34dd65c935509024","type":"group","x":2200,"y":3440,"width":1489,"height":1183,"color":"3","label":"3.2 Matrix Algebra"},
		{"id":"d9ab7a9a54603d1e","type":"group","x":6086,"y":3468,"width":1783,"height":913,"color":"1","label":"Elementary Matrices"},
		{"id":"c77a696822b8dd89","type":"group","x":1262,"y":9121,"width":1924,"height":720,"label":"row, col, null"},
		{"id":"99587d1145bd6959","type":"group","x":603,"y":5940,"width":1820,"height":660,"color":"1","label":"Not Subspace"},
		{"id":"3b688c83e433b837","type":"group","x":7126,"y":6729,"width":1260,"height":777,"label":"行列の話"},
		{"id":"0490757fcfcc3941","type":"group","x":6446,"y":6729,"width":674,"height":1091,"label":"ベクトル空間の話"},
		{"id":"a464e1423303998b","type":"group","x":1243,"y":6000,"width":1120,"height":580,"label":"加法，スカラー倍に閉じてない / (2)を満たさない"},
		{"id":"6dd46e5a738a70af","x":8980,"y":3260,"width":600,"height":640,"color":"3","type":"group","label":"4.1 Intro"},
		{"id":"52ac00b08028f12b","type":"group","x":643,"y":6000,"width":580,"height":580,"label":"原点を含めない / (1)を満たさない"},
		{"id":"7c32ab6fff3ed132","type":"group","x":603,"y":6740,"width":506,"height":630,"color":"4","label":"Subspace"},
		{"id":"b0fa401e849cc3e0","type":"text","text":"> [!tip]\n> ### Property of Transpose\n> Let $A$ and $B$ be matrices (whose sizes are such that the indicated operations can be performed) \n> Let $k$ be a scalar. \n> Then:<br>\n> \n> a. $(A^T)^T = A$\n> \n> b. $(A + B)^T = A^T + B^T$\n> \n> c. $(kA)^T = k(A^T)$\n> \n> > [!Warning]\n> > d, e は線型代数特有の面白い性質\n> \n>  d. $(AB)^T = B^T A^T$\n> \n> e. $(A^r)^T = (A^T)^r \\quad \\text{for all nonnegative integers } r$","x":2230,"y":4043,"width":815,"height":560},
		{"id":"7162ccdb2f765f6a","type":"text","text":"> [!tip] Theorem 3.7\n> #### Solution of Linear Systems with an Invertible Matrix\n> If $A$ is an invertible $n \\times n$ matrix, then the system of linear equations given by:\n> \n> $$\n> A\\mathbf{x} = \\mathbf{b}\n> $$\n> \n> has the unique solution:\n> \n> $$\n> \\mathbf{x} = A^{-1}\\mathbf{b}\n> $$\n> \n> for any $\\mathbf{b}$ in $\\mathbb{R}^n$.\n","x":4426,"y":3656,"width":599,"height":360},
		{"id":"0625cfabd09b3b41","type":"text","text":"> [!tip] Theorem 3.8\n> #### Inverse of a 2x2 Matrix\n> If \n> $$\n> A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix},\n> $$\n> then $A$ is invertible if $ad - bc \\neq 0$, in which case:\n> \n> $$\n> A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}.\n> $$\n> \n> If $ad - bc = 0$, then $A$ is not invertible.\n","x":4426,"y":4051,"width":440,"height":383},
		{"id":"7e0b631421f2762f","type":"text","text":"> [!cite] Proof\n> 方程式 $A\\mathbf{x} = \\mathbf{b}$ に解があるってことと、その解が一つだけやってことを証明するで。\n> \n> **1. 解が存在することを証明するで！**\n> まず、$\\mathbf{x} = A^{-1}\\mathbf{b}$ が正しいかどうか確かめるわな：\n> \n> $$\n> A\\mathbf{x} = A(A^{-1}\\mathbf{b}) = (AA^{-1})\\mathbf{b} = I\\mathbf{b} = \\mathbf{b}.\n> $$\n> \n> これで、$\\mathbf{x} = A^{-1}\\mathbf{b}$ はちゃんと $A\\mathbf{x} = \\mathbf{b}$ を満たしてるから、解は**少なくともひとつある**ってことや。\n> \n> **2. 解が一つだけやってことを証明するで！**\n> 次に、もし別の解があると仮定して、それを $\\mathbf{y}$ としてみるわな。つまり、$A\\mathbf{y} = \\mathbf{b}$ やねん。ほんなら両辺に $A^{-1}$ をかけるで：\n> \n> $$\n> A^{-1}(A\\mathbf{y}) = A^{-1}\\mathbf{b}.\n> $$\n> \n> これを簡単にするとこうなる：\n> \n> $$\n> (A^{-1}A)\\mathbf{y} = A^{-1}\\mathbf{b} \\implies I\\mathbf{y} = A^{-1}\\mathbf{b} \\implies \\mathbf{y} = A^{-1}\\\n","x":5025,"y":4033,"width":920,"height":480},
		{"id":"8289f5873de50f99","type":"text","text":"**例3: $n \\times n$ 行列の集合**  \n- 部分空間:\n  $$\n  S = \\{A \\in M_{n \\times n} \\mid A^T = -A\\}\n  $$\n  （$S$ は全てのtranspose matrixの集合）\n- **理由**:\n  1. ゼロ行列はtransposeなので $S$ に含まれる。\n  2. 加法に閉じている: transpose同士の和もtranspose。\n  3. スカラー倍に閉じている: スカラー倍されたtranspose行列もtranspose。","x":1829,"y":7005,"width":560,"height":340},
		{"id":"3e84058ca8561bd3","type":"text","text":"##### null spaceの基底を求める\n\n> [!example] Example 3.48\n> \n> Problem\n> Find a basis for the **null space** of the matrix $A$ from [Example 3.47](#example-3-47), where:\n> $$\n> A =\n> \\begin{bmatrix}\n> 1 & 1 & 3 & 1 & 6 \\\\\n> 2 & -1 & 0 & 0 & -1 \\\\\n> -3 & 2 & 1 & -2 & 1 \\\\\n> 4 & 1 & 6 & 1 & 3\n> \\end{bmatrix}.\n> $$\n> \n> Solution\n> To find a basis for the null space of $A$, we solve the homogeneous system $A\\mathbf{x} = \\mathbf{0}$. Begin by forming the augmented matrix:\n> $$\n> [A | 0] =\n> \\begin{bmatrix}\n> 1 & 1 & 3 & 1 & 6 & 0 \\\\\n> 2 & -1 & 0 & 0 & -1 & 0 \\\\\n> -3 & 2 & 1 & -2 & 1 & 0 \\\\\n> 4 & 1 & 6 & 1 & 3 & 0\n> \\end{bmatrix}.\n> $$\n> \n> Next, row reduce the augmented matrix to its reduced row echelon form (RREF):\n> $$\n> [A | 0] \\sim\n> \\begin{bmatrix}\n> 1 & 0 & 1 & 0 & -1 & 0 \\\\\n> 0 & 1 & 2 & 0 & 3 & 0 \\\\\n> 0 & 0 & 0 & 1 & 4 & 0 \\\\\n> 0 & 0 & 0 & 0 & 0 & 0\n> \\end{bmatrix}.\n> $$\n> \n> > [!info] ここから先が新しい\n> \n> From the RREF, we write the system of equations:\n> $$\n> x_1 + x_3 - x_5 = 0, \\\\\n> x_2 + 2x_3 + 3x_5 = 0, \\\\\n> x_4 + 4x_5 = 0.\n> $$\n> \n> Expressing $x_1$, $x_2$, and $x_4$ in terms of the free variables $x_3$ and $x_5$:\n> $$\n> x_1 = -x_3 + x_5, \\\\\n> x_2 = -2x_3 - 3x_5, \\\\\n> x_4 = -4x_5.\n> $$\n> \n> Thus, the solution vector $\\mathbf{x}$ can be written as:\n> $$\n> \\mathbf{x} =\n> x_3\n> \\begin{bmatrix}\n> -1 \\\\\n> -2 \\\\\n> 1 \\\\\n> 0 \\\\\n> 0\n> \\end{bmatrix}\n> + x_5\n> \\begin{bmatrix}\n> 1 \\\\\n> -3 \\\\\n> 0 \\\\\n> -4 \\\\\n> 1\n> \\end{bmatrix}.\n> $$\n> \n> The null space is spanned by the following vectors, which form a basis for the null space:\n> $$\n> \\left\\{\n> \\begin{bmatrix}\n> -1 \\\\\n> -2 \\\\\n> 1 \\\\\n> 0 \\\\\n> 0\n> \\end{bmatrix},\n> \\begin{bmatrix}\n> 1 \\\\\n> -3 \\\\\n> 0 \\\\\n> -4 \\\\\n> 1\n> \\end{bmatrix}\n> \\right\\}.\n> $$\n> ","x":5546,"y":7298,"width":680,"height":1200},
		{"id":"63fa56a516280068","type":"text","text":"> [!tip] Theorem 3.23: The Basis Theorem\n> \n> Let $S$ be a subspace of $\\mathbb{R}^n$.\n> Then any two bases for $S$ have the same number of vectors.\n","x":3950,"y":8509,"width":536,"height":165},
		{"id":"1f99904c1ee13df7","type":"text","text":"Ax = bにおいて, 行列Aが実数からなる場合，\nxは3パターンからなる\n\n- xに解なし\n\t- 矛盾\n- xにユニークな解がある\n\t- 線型独立\n- xに無限の解がある\n\t- 線形従属\n\n> [!question]\n> ここをふかぼる必要がある","x":2957,"y":8095,"width":433,"height":387},
		{"id":"ed540d9732ecb6be","type":"text","text":"#### Column space\n##### Problem\nDetermine whether $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ is in the column space of \n$$\nA = \\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\\\ 3 & -3 \\end{bmatrix}.\n$$\n\n##### Solution\nBy **Theorem 2.4**, $\\mathbf{b}$ is a linear combination of the columns of $A$ if and only if the linear system $A \\mathbf{x} = \\mathbf{b}$ is consistent. We row reduce the augmented matrix as follows:\n\n$$\n\\begin{bmatrix} 1 & -1 & 1 \\\\ 0 & 1 & 2 \\\\ 3 & -3 & 3 \\end{bmatrix} \n\\longrightarrow \n\\begin{bmatrix} 1 & 0 & 3 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix}.\n$$\n\nThe system is consistent (and has a unique solution). Therefore, $\\mathbf{b}$ is in $\\text{col}(A)$.","x":1314,"y":9161,"width":660,"height":508},
		{"id":"feee36a19b552471","type":"text","text":"#### Row Space\n\n##### Problem:\nDetermine if $\\mathbf{w} = [4 \\, 5]$ is in the row space of $A$.\n\nGiven:\n$$\nA =\n\\begin{bmatrix}\n1 & -1 \\\\\n0 & 1 \\\\\n3 & -3\n\\end{bmatrix}, \\quad\n\\mathbf{w} = [4 \\, 5].\n$$\n\n##### Solution\nTo check if $\\mathbf{w}$ is in $\\text{row}(A)$, augment $A$ with $\\mathbf{w}$:\n$$\n\\begin{bmatrix}\n1 & -1 \\\\\n0 & 1 \\\\\n3 & -3 \\\\\n4 & 5\n\\end{bmatrix}\n\\to\n\\begin{bmatrix}\n1 & -1 \\\\\n0 & 1 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{bmatrix}.\n$$\n\nSince the augmented matrix reduces consistently, $\\mathbf{w}$ is a linear combination of the rows of $A$.\n\nTherefore, \n$$\\mathbf{w} \\in \\text{row}(A)$$\n","x":1986,"y":9161,"width":589,"height":633},
		{"id":"b1ca56510c7dcb45","type":"text","text":"#### Null Space\n\n##### Problem:\nDetermine if $\\mathbf{x} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$ is in the null space of $A$.\n\nGiven:\n$$\nA =\n\\begin{bmatrix}\n1 & -1 & 2 \\\\\n0 & 2 & -4 \\\\\n3 & -3 & 6\n\\end{bmatrix}.\n$$\n\n##### Solution:\nTo check if $\\mathbf{x}$ is in $\\text{null}(A)$, compute $A\\mathbf{x}$:\n$$\nA \\mathbf{x} =\n\\begin{bmatrix}\n1 & -1 & 2 \\\\\n0 & 2 & -4 \\\\\n3 & -3 & 6\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\ -1 \\\\ 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1(1) + (-1)(-1) + 2(2) \\\\\n0(1) + 2(-1) + (-4)(2) \\\\\n3(1) + (-3)(-1) + 6(2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n6 \\\\ -10 \\\\ 21\n\\end{bmatrix}.\n$$\n\nSince $A\\mathbf{x} \\neq \\mathbf{0}$, $\\mathbf{x}$ is **not** in the null space of $A$.\n\n**Conclusion:**\n$\\mathbf{x} \\notin \\text{null}(A)$.\n","x":2586,"y":9161,"width":560,"height":604},
		{"id":"32b8ca1c9dae8ad6","type":"text","text":"**例4: 多項式の空間**  \n- 部分空間:\n  $$\n  S = \\{p(x) \\in P_3 \\mid p(0) = 0\\}\n  $$\n  > [!warning] （3次以下の多項式で定数項が0のもの）\n- **理由**:\n  1. ゼロ多項式 $p(x) = 0$ は $S$ に含まれる。\n  2. 加法に閉じている: 2つの多項式を足しても定数項は0。\n  3. スカラー倍に閉じている: スカラー倍しても定数項は0。","x":1889,"y":7361,"width":500,"height":340},
		{"id":"88584b7db335d6ac","type":"text","text":"> [!tip] Theorem 3.21\n> \n> Let $A$ be an $m \\times n$ matrix and let $N$ be the set of solutions of the homogeneous linear system $A\\mathbf{x} = \\mathbf{0}$. Then $N$ is a subspace of $\\mathbb{R}^n$.\n\nhomogeneous, 唯一","x":1859,"y":7887,"width":644,"height":223},
		{"id":"aaeeed1e2b5b4a8b","type":"text","text":"> [!note] Definition\n> \n> #### null space\n> Let $A$ be an $m \\times n$ matrix. The **null space** of $A$ is the subspace of $\\mathbb{R}^n$ consisting of solutions of the homogeneous linear system $A\\mathbf{x} = \\mathbf{0}$. It is denoted by $\\text{null}(A)$.\n","x":1885,"y":8203,"width":592,"height":231},
		{"id":"7a7ab5e8b1fbe002","type":"text","text":"**Row Space**","x":849,"y":7114,"width":186,"height":60},
		{"id":"04bb1535b901137d","type":"text","text":"> [!tip] Theorem 3.20\n> Let $B$ be any matrix that is row equivalent to a matrix $A$. Then \n> \n> $$\n> \\text{row}(B) = \\text{row}(A).\n> $$\n> \n\n行基本変形を行った結果が同じなら，Row space","x":2786,"y":8591,"width":521,"height":249},
		{"id":"d0a5da3d7d0b449c","type":"text","text":"> [!warning] （$\\mathbb{R}^2$ 内）\n\n原点を通る**直線**","x":849,"y":6907,"width":186,"height":107},
		{"id":"77058909374dc990","type":"text","text":"> [!theorem] Theorem 3.22\n> Let $A$ be a matrix whose entries are real numbers.\n> For any system of linear equations $A\\mathbf{x} = \\mathbf{b}$, exactly one of the following is true:\n> \n> a. There is no solution.\n> \n> b. There is a unique solution.\n> \n> c. There are infinitely many solutions.\n","x":2586,"y":7887,"width":576,"height":314},
		{"id":"bd265e541203289d","type":"text","text":"**Symmetry Matrix からなる空間**","x":1503,"y":7303,"width":281,"height":60},
		{"id":"062dd0c345264ccf","type":"text","text":"  $$\n  S = \\{(x, 2x) \\mid x \\in \\mathbb{R}\\}k\n$$\n- **理由**:\n  1. ゼロベクトル $(0, 0)$ を含む。\n  2. 加法に閉じている: $(a, 2a) + (b, 2b) = (a+b, 2(a+b)) \\in S$。\n  3. スカラー倍に閉じている: $c(a, 2a) = (ca, 2ca) \\in S$。\n","x":1260,"y":6930,"width":485,"height":214},
		{"id":"921e676f8c0c19d5","type":"text","text":"### Row space / Column space\n> [!note] Definition\n> Let $A$ be an $m \\times n$ matrix.\n> \n> #### row space\n> a. The **row space** of $A$ is the subspace $\\text{row}(A)$ of $\\mathbb{R}^n$ spanned by the rows of $A$.\n> > [!example] row(A)の意味\n> > $\\text{row}(A) = \\text{span} \\{ \\mathbf{r}_1, \\mathbf{r}_2, \\dots, \\mathbf{r}_m \\}$,\n> \n> #### column space\n> b. The **column space** of $A$ is the subspace $\\text{col}(A)$ of $\\mathbb{R}^m$ spanned by the columns of $A$.\n> > [!example] col(A)の意味\n> > $\\text{col}(A) = \\text{span} \\{ \\mathbf{c}_1, \\mathbf{c}_2, \\dots, \\mathbf{c}_m \\}$,\n","x":879,"y":7978,"width":765,"height":613},
		{"id":"4ded22cbe82a3ab3","type":"text","text":"**Null Space**","x":849,"y":7274,"width":186,"height":60},
		{"id":"b873c13ca87064e6","type":"text","text":"その他","x":849,"y":7034,"width":125,"height":60},
		{"id":"a2f0939b69b67762","type":"text","text":"> [!warning] （$\\mathbb{R}^3$ 内）\n\n原点を通る**平面**  \n","x":849,"y":6780,"width":185,"height":107},
		{"id":"878cd89d922f980f","type":"text","text":"**Column Space**","x":849,"y":7194,"width":186,"height":60},
		{"id":"0b6604b2fd5a7de8","type":"text","text":"**例1: 原点を通らない直線（$\\mathbb{R}^2$ 内）**  \n- 例えば:\n  $$\n  S = \\{(x, 2x+1) \\mid x \\in \\mathbb{R}\\}\n  $$\n- **理由**:\n  - ゼロベクトル $(0, 0)$ を含まないため部分空間ではない。","x":663,"y":6020,"width":534,"height":220},
		{"id":"b507e8483e1ef16a","type":"text","text":"加法に閉じてない．","x":1263,"y":6020,"width":250,"height":60},
		{"id":"6f14310a415cce9a","type":"text","text":"**例2: 平面 $z=1$ （$\\mathbb{R}^3$ 内）**  \n- 例えば:\n  $$\n  S = \\{(x, y, 1) \\mid x, y \\in \\mathbb{R}\\}\n  $$\n- **理由**:\n  - ゼロベクトル $(0, 0, 0)$ を含まないため部分空間ではない。\n  - 加法に閉じていない: $(1, 1, 1) + (1, 1, 1) = (2, 2, 2) \\notin S$。\n","x":663,"y":6274,"width":530,"height":292},
		{"id":"28035385817fbc35","type":"text","text":"#### Matrix Transformation as a Linear Transformation\n> [!tip] Theorem 3.30:\n> \n> Let $A$ be an $m \\times n$ matrix. Then the matrix transformation\n> $$\n> T_A : \\mathbb{R}^n \\to \\mathbb{R}^m\n> $$\n> defined by\n> $$\n> T_A(\\mathbf{x}) = A\\mathbf{x} \\quad \\text{(for $\\mathbf{x}$ in $\\mathbb{R}^n$)}\n> $$\n> is a linear transformation.\n> ","x":1100,"y":10240,"width":540,"height":345},
		{"id":"e3b3efdc79268161","type":"text","text":"# 回転の定義と行列導出 - Example 3.58\n> [!info]\n> ## 1. 回転の定義と線形性の確認\n> 回転 $R_\\theta$ を、原点を中心として角度 $\\theta$ だけ回転させる変換として定義します。これが線形変換であることを確認するために、以下の性質を示します。\n> \n> > [!cite] 加法\n> > ### a. ベクトルの加法に関して：\n> > ![[standard_linear_figure_3.6.png]]\n> > 図3.6(a)と(b)を使って、ベクトル $\\mathbf{u}$ と $\\mathbf{v}$ の和 $\\mathbf{u} + \\mathbf{v}$ を考えます。平行四辺形の対角線として、この和が作られます。回転 $R_\\theta$ を加えた後も、この関係が成り立つため、以下が成立します：\n> > \n> > $$\n> > R_\\theta(\\mathbf{u} + \\mathbf{v}) = R_\\theta(\\mathbf{u}) + R_\\theta(\\mathbf{v})\n> > $$\n> \n> > [!cite] スカラー倍\n> > ### b. スカラー倍に関して：\n> > ![[standard_linear_figure_3.7.png]]\n> > ベクトル $\\mathbf{v}$ をスカラー倍した $c\\mathbf{v}$ の回転 $R_\\theta(c\\mathbf{v})$ を考えます。この回転の結果は、スカラー $c$ をそのまま維持して、回転されたベクトルに適用されるので、\n> > \n> > $$\n> > R_\\theta(c\\mathbf{v}) = cR_\\theta(\\mathbf{v})\n> > $$\n\nこれにより、$R_\\theta$ は線形変換であると確認されます。\n\n---\n\n> [!info]\n> ## 2. 標準基底の回転と行列の導出\n> 回転行列を導出するには、標準基底ベクトル $\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ と $\\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ を回転した結果を調べます。\n> \n> ### a. $R_\\theta(\\mathbf{e}_1)$ の結果：\n> ![[standard_linear_figure_3.8_e1.png]]\n> 図3.8を参照すると、$\\mathbf{e}_1$ は回転後に以下の位置に移動します：\n> \n> $$\n> R_\\theta(\\mathbf{e}_1) = \\begin{bmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{bmatrix}\n> $$\n> \n> ### b. $R_\\theta(\\mathbf{e}_2)$ の結果：\n> ![[standard_linear_figure_3.9_e2.png]]\n> 図3.9を参照すると、$\\mathbf{e}_2$ は回転後に以下の位置に移動します：\n> \n> $$\n> R_\\theta(\\mathbf{e}_2) = \\begin{bmatrix} -\\sin\\theta \\\\ \\cos\\theta \\end{bmatrix}\n> $$\n\n---\n\n## 3. 回転行列の完成\n以上より、回転行列 $R_\\theta$ は、標準基底の変換結果を列に持つ行列として次のように表されます：\n\n$$\nR_\\theta = \\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{bmatrix}\n$$\n\nこの行列は、原点を中心とした角度 $\\theta$ の反時計回りの回転を表します。\n","x":2380,"y":10655,"width":1380,"height":2125},
		{"id":"72553d0c109373d9","type":"text","text":"![[Projection_Matrix_R2_to_R2_Linear_Transformation.jpeg.jpeg]]","x":3780,"y":10655,"width":1280,"height":1278},
		{"id":"6084a5f3452448b6","type":"text","text":"#### Transformation の合成\n\n> [!tip] Theorem 3.32\n> Let $T : \\mathbb{R}^m \\to \\mathbb{R}^n$ and $S : \\mathbb{R}^n \\to \\mathbb{R}^p$ be linear transformations. Then $S \\circ T : \\mathbb{R}^m \\to \\mathbb{R}^p$ is a linear transformation. Moreover, their standard matrices are related by:\n> $$\n> S \\circ T = [S][T](\\mathbf{v}) = S(T(\\mathbf{v}))\n> $$","x":5166,"y":10240,"width":800,"height":280,"color":"3"},
		{"id":"c2338f1ffd1ab8e9","type":"text","text":"#### Inverses of Linear Transformations\n> [!note] Definition\n> \n> Let $S$ and $T$ be linear transformations from $\\mathbb{R}^n$ to $\\mathbb{R}^n$. Then $S$ and $T$ are **inverse transformations** if:\n> \n> - $S \\circ T = I_n$\n> - $T \\circ S = I_n$\n> \n\n> [!tip] Theorem 3.33\n> \n> Let $T : \\mathbb{R}^n \\to \\mathbb{R}^n$ be an invertible linear transformation. Then its standard matrix $[T]$ is an invertible matrix, and:\n> \n> $$\n> [T^{-1}] = [T]^{-1}\n> $$\n","x":5166,"y":10550,"width":780,"height":460},
		{"id":"87c0aeea841a31ec","type":"text","text":"##### 反射変換(Reflection Transformation)\n> [!example] Example 3.56\n> \n> **Problem**\n> Let $F : \\mathbb{R}^2 \\to \\mathbb{R}^2$ be the transformation that sends each point to its reflection in the $x$-axis. Show that $F$ is a linear transformation.\n> \n> **Solution**\n> From [Figure 3.4](#), it is clear that $F$ sends the point $(x, y)$ to the point $(x, -y)$. Thus, we may write:\n> \n> $$\n> F\\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}x \\\\ -y\\end{bmatrix}.\n> $$\n> \n> We could proceed to check that $F$ is linear, as in [Example 3.55](#) (this one is even easier to check!), but it is faster to observe that:\n> \n> $$\n> \\begin{bmatrix}x \\\\ -y\\end{bmatrix} = x\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + y\\begin{bmatrix}0 \\\\ -1\\end{bmatrix} = \\begin{bmatrix}1 & 0 \\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix}x \\\\ y\\end{bmatrix}.\n> $$\n> \n> Therefore, $F\\begin{bmatrix}x \\\\ y\\end{bmatrix} = A\\begin{bmatrix}x \\\\ y\\end{bmatrix}$, where \n> \n> $$\n> A = \\begin{bmatrix}1 & 0 \\\\ 0 & -1\\end{bmatrix},\n> $$ \n> \n> so $F$ is a matrix transformation. It now follows, by [Theorem 3.30](#), that $F$ is a linear transformation.","x":1144,"y":10655,"width":560,"height":665},
		{"id":"0d9d8a10a260ce79","type":"text","text":"### Matrix Powers","x":540,"y":4047,"width":207,"height":84},
		{"id":"a0e17faa77656cf4","type":"text","text":"#### a. Inductionで解くパターン（王道）時間かかる\n> [!example]\n> Let A be a matrix\n> \n> $$\n> A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix},\n> $$\n> \n> then:\n> \n> $$\n> A^2 = A \\cdot A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix},\n> $$\n> \n> $$\n> A^3 = A^2 \\cdot A = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 4 & 4 \\\\ 4 & 4 \\end{bmatrix}.\n> $$\n> \n> In general:\n> \n> $$\n> A^n = \\begin{bmatrix} 2^{n-1} & 2^{n-1} \\\\ 2^{n-1} & 2^{n-1} \\end{bmatrix} \\quad \\text{for all } n \\geq 1.\n> $$\n> \n> ---\n> \n> ##### Proof by Mathematical Induction:\n> \n> 1. **Base Case (\\(n = 1\\))**:  \n>    When \\(n = 1\\),\n> \n>    $$\n>    A^1 = \\begin{bmatrix} 2^{1-1} & 2^{1-1} \\\\ 2^{1-1} & 2^{1-1} \\end{bmatrix} = \\begin{bmatrix} 2^0 & 2^0 \\\\ 2^0 & 2^0 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = A.\n>    $$\n> \n>    Thus, the base case holds.\n> \n> 2. **Induction Hypothesis:**  \n>    Assume that the formula holds for \\(n = k\\), i.e.,\n> \n>    $$\n>    A^k = \\begin{bmatrix} 2^{k-1} & 2^{k-1} \\\\ 2^{k-1} & 2^{k-1} \\end{bmatrix}.\n>    $$\n> \n> 3. **Induction Step:**  \n>    To prove the formula holds for \\(n = k+1\\), compute \\(A^{k+1} = A^k \\cdot A\\):\n> \n>    $$\n>    A^{k+1} = \\begin{bmatrix} 2^{k-1} & 2^{k-1} \\\\ 2^{k-1} & 2^{k-1} \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}.\n>    $$\n> \n>    Performing the matrix multiplication:\n> \n>    $$\n>    A^{k+1} = \\begin{bmatrix} 2^{k} & 2^{k} \\\\ 2^{k} & 2^{k} \\end{bmatrix}.\n>    $$\n> \n>    Simplifying further:\n> \n>    $$\n>    A^{k+1} = \\begin{bmatrix} 2^{(k+1)-1} & 2^{(k+1)-1} \\\\ 2^{(k+1)-1} & 2^{(k+1)-1} \\end{bmatrix}.\n>    $$\n> \n> Thus, the formula holds for all \\(n \\geq 1\\) by the principle of mathematical induction.\n","x":1184,"y":4055,"width":700,"height":1240},
		{"id":"a77d8099dc1ff00d","type":"text","text":"#### b. 循環が見つかるパターン\n\n> [!example]\n> If \n> $$\n> B = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix},\n> $$\n> \n> then:\n> \n> $$\n> B^2 = B \\cdot B = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n> $$\n> \n> Continuing, we find:\n> \n> $$\n> B^3 = B^2 \\cdot B = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix},\n> $$\n> \n> and:\n> \n> $$\n> B^4 = B^3 \\cdot B = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n> $$\n> \n> Thus:\n> \n> $$\n> B^5 = B, \n> $$\n> \n> and the sequence of powers of \\( B \\) repeats in a cycle of four:\n> \n> $$\n> \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}, \\quad\n> \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}, \\quad\n> \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}, \\quad\n> \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\dots\n> $$\n","x":540,"y":4608,"width":539,"height":687},
		{"id":"d9cccd556dd0287f","type":"text","text":"## 3.2 Matrix Algebra\n### 代数的な性質\n","x":2220,"y":3460,"width":272,"height":91},
		{"id":"3a0926d41e1f6fdf","type":"text","text":"**例3: スカラー倍に閉じていない集合**  \n- 例えば:\n  $$\n  S = \\{(x, y) \\in \\mathbb{R}^2 \\mid x + y = 1\\}\n  $$\n- **理由**:\n  - スカラー倍に閉じていない: $(1, 0) \\in S$ だが $2(1, 0) = (2, 0) \\notin S$。","x":1263,"y":6102,"width":540,"height":276},
		{"id":"d1a14f4994db99e3","type":"text","text":"  $$\n  S = \\{(x, y, 0) \\mid x, y \\in \\mathbb{R}\\}\n  $$\n- **理由**:\n  1. ゼロベクトル $(0, 0, 0)$ を含む。\n  2. 加法に閉じている: $(x_1, y_1, 0) + (x_2, y_2, 0) = (x_1+x_2, y_1+y_2, 0) \\in S$。\n  3. スカラー倍に閉じている: $c(x, y, 0) = (cx, cy, 0) \\in S$。","x":1254,"y":6680,"width":496,"height":241},
		{"id":"0bf9d7f28207dc81","type":"text","text":"**$\\mathbb{R}^n$**\n\n最大のSubspace","x":623,"y":6780,"width":180,"height":91},
		{"id":"0576c9e18978abc0","type":"text","text":"**零ベクトル      $\\mathbb{0}$**\n最小のSubspace","x":623,"y":6891,"width":186,"height":91},
		{"id":"f762fb88b5ab3176","type":"text","text":"##### 回転変換(Rotation Transformation)\n> [!example] Example 3.57\n> \n> **Problem**  \n> Let $R : \\mathbb{R}^2 \\to \\mathbb{R}^2$ be the transformation that rotates each point $90^\\circ$ counterclockwise about the origin. Show that $R$ is a linear transformation.\n> \n> **Solution**  \n> The transformation $R$ can be represented as a matrix multiplication. A $90^\\circ$ counterclockwise rotation about the origin is defined by the following transformation matrix:\n> \n> $$\n> R\\begin{bmatrix}x \\\\ y\\end{bmatrix} =\n> \\begin{bmatrix}\n> 0 & -1 \\\\\n> 1 & 0\n> \\end{bmatrix}\n> \\begin{bmatrix}x \\\\ y\\end{bmatrix}.\n> $$\n> \n> 1. **Addition property:**  \n>    Let $\\mathbf{u} = \\begin{bmatrix}u_1 \\\\ u_2\\end{bmatrix}$ and $\\mathbf{v} = \\begin{bmatrix}v_1 \\\\ v_2\\end{bmatrix}$. Then:\n> \n>    $$\n>    R(\\mathbf{u} + \\mathbf{v}) =\n>    \\begin{bmatrix}\n>    0 & -1 \\\\\n>    1 & 0\n>    \\end{bmatrix}\n>    \\begin{bmatrix}\n>    u_1 + v_1 \\\\\n>    u_2 + v_2\n>    \\end{bmatrix}.\n>    $$\n> \n>    By matrix multiplication, this becomes:\n> \n>    $$\n>    R(\\mathbf{u} + \\mathbf{v}) =\n>    \\begin{bmatrix}\n>    0 & -1 \\\\\n>    1 & 0\n>    \\end{bmatrix}\n>    \\begin{bmatrix}\n>    u_1 \\\\\n>    u_2\n>    \\end{bmatrix}\n>    +\n>    \\begin{bmatrix}\n>    0 & -1 \\\\\n>    1 & 0\n>    \\end{bmatrix}\n>    \\begin{bmatrix}\n>    v_1 \\\\\n>    v_2\n>    \\end{bmatrix}.\n>    $$\n> \n>    Hence, $R(\\mathbf{u} + \\mathbf{v}) = R(\\mathbf{u}) + R(\\mathbf{v})$.\n> \n> 2. **Scalar multiplication property:**  \n>    Let $c$ be a scalar and $\\mathbf{u} = \\begin{bmatrix}u_1 \\\\ u_2\\end{bmatrix}$. Then:\n> \n>    $$\n>    R(c\\mathbf{u}) =\n>    \\begin{bmatrix}\n>    0 & -1 \\\\\n>    1 & 0\n>    \\end{bmatrix}\n>    \\begin{bmatrix}\n>    cu_1 \\\\\n>    cu_2\n>    \\end{bmatrix}.\n>    $$\n> \n>    By distributing $c$, this becomes:\n> \n>    $$\n>    cR(\\mathbf{u}) =\n>    c\n>    \\begin{bmatrix}\n>    0 & -1 \\\\\n>    1 & 0\n>    \\end{bmatrix}\n>    \\begin{bmatrix}\n>    u_1 \\\\\n>    u_2\n>    \\end{bmatrix}.\n>    $$\n> \n>    Hence, $R(c\\mathbf{u}) = cR(\\mathbf{u})$.\n> \n> Since both properties are satisfied, $R$ is a linear transformation.\n> ","x":1736,"y":10655,"width":620,"height":970},
		{"id":"d9278b565480d376","type":"text","text":"> [!note] Definition\n> A **matrix** is a rectangular array of numbers called the **entries**, or **elements**, of the matrix.\n","x":-216,"y":3682,"width":393,"height":166},
		{"id":"6c9983081b02a650","type":"text","text":"### Matrix Multiplication\n\n> [!note] Definition\n> If $A$ is an $m \\times n$ matrix and $B$ is an $n \\times r$ matrix, then the **product** $C = AB$ is an $m \\times r$ matrix. \n> The $(i, j)$ entry of the product is computed as follows:\n> \n> $$\n> c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \\cdots + a_{in}b_{nj}.\n> $$\n\n> [!example] Example 3.6\n> Compute $AB$ if:\n> \n> $$\n> A = \\begin{bmatrix} 1 & 3 & -1 \\\\ -2 & -1 & 1 \\end{bmatrix}, \\quad\n> B = \\begin{bmatrix} -4 & 0 & 3 & -1 \\\\ 5 & -2 & -1 & 1 \\\\ -1 & 2 & 0 & 6 \\end{bmatrix}.\n> $$\n> \n> Then:\n> \n> $$\n> AB = \\begin{bmatrix} 12 & -8 & 0 & -4 \\\\ 2 & 4 & -5 & 7 \\end{bmatrix}.\n> $$\n","x":-216,"y":4000,"width":563,"height":551},
		{"id":"e02bc77a41174546","type":"text","text":"> [!example] Example 3.2\n> Consider the matrices:\n> \n> $$\n> R = [1 \\ 4 \\ 3]\n> $$\n> \n> and\n> \n> $$\n> C = \\begin{bmatrix} 1 \\\\ 4 \\\\ 3 \\end{bmatrix}.\n> $$\n> \n> Despite the fact that $R$ and $C$ have the same entries in the same order, $R \\neq C$ since $R$ is $1 \\times 3$ and $C$ is $3 \\times 1$. (If we read $R$ and $C$ aloud, they both sound the same: “one, four, three.”) Thus, our distinction between row matrices/vectors and column matrices/vectors is an important one.\n","x":284,"y":3460,"width":528,"height":445},
		{"id":"7baa262b93ca3fb0","type":"text","text":"### Scalar Multiplication\n> [!example] Example 3.4\n> For matrix $A$ in [Example 3.3](#example-3.3),\n> \n> $$\n> 2A = \\begin{bmatrix} 2 & 8 & 0 \\\\ -4 & 12 & 10 \\end{bmatrix}, \\quad\n> \\frac{1}{2}A = \\begin{bmatrix} \\frac{1}{2} & 2 & 0 \\\\ -1 & 3 & \\frac{5}{2} \\end{bmatrix}, \\quad\n> \\text{and} \\quad (-1)A = \\begin{bmatrix} -1 & -4 & 0 \\\\ 2 & -6 & -5 \\end{bmatrix}.\n> $$\n","x":1480,"y":3760,"width":640,"height":240},
		{"id":"1fe99bf9f314d746","type":"text","text":"### Transpose\n> [!example] Example 3.14\n> Let\n> \n> $$\n> A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 5 & 0 & 1 \\end{bmatrix}, \\quad\n> B = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\quad\n> \\text{and} \\quad C = \\begin{bmatrix} 5 & -1 & 2 \\end{bmatrix}.\n> $$\n> \n> Then their transposes are:\n> \n> $$\n> A^T = \\begin{bmatrix} 1 & 5 \\\\ 3 & 0 \\\\ 2 & 1 \\end{bmatrix}, \\quad\n> B^T = \\begin{bmatrix} a & c \\\\ b & d \\end{bmatrix}, \\quad\n> \\text{and} \\quad C^T = \\begin{bmatrix} 5 \\\\ -1 \\\\ 2 \\end{bmatrix}.\n> $$\n","x":-216,"y":4620,"width":646,"height":380},
		{"id":"8219f831696f38e6","type":"text","text":"### Symmetric\n> [!note] Definition\n> A square matrix $A$ is **symmetric** if $A^T = A$ — that is, if $A$ is equal to its own transpose.\n","x":-216,"y":5040,"width":445,"height":200},
		{"id":"2a12d16ef3f7ffb3","type":"text","text":"> [!example]\n> $$\n> A = \\begin{bmatrix} 1 & 4 & 0 \\\\ -2 & 6 & 5 \\end{bmatrix}, \\quad\n> B = \\begin{bmatrix} -3 & 1 & -1 \\\\ 3 & 0 & 2 \\end{bmatrix}, \\quad \\text{and} \\quad\n> C = \\begin{bmatrix} 4 & 3 \\\\ 2 & 1 \\end{bmatrix}.\n> $$\n","x":864,"y":3640,"width":508,"height":140},
		{"id":"4b85b11f1d1af140","type":"text","text":"### Addition\n\n> [!example] Example 3.3\n> $$\n> A + B = \\begin{bmatrix} -2 & 5 & -1 \\\\ 1 & 6 & 7 \\end{bmatrix}.\n> $$\n> \n> but neither $A + C$ nor $B + C$ is defined.\n","x":1480,"y":3480,"width":440,"height":240},
		{"id":"748fca38233d13fd","type":"text","text":"**例4: 整数だけを含む集合**  \n- 例えば:\n  $$\n  S = \\{(x, y) \\in \\mathbb{R}^2 \\mid x, y \\in \\mathbb{Z}\\}\n  $$\n- **理由**:\n  - スカラー倍に閉じていない: $(1, 1) \\in S$ だが $\\frac{1}{2}(1, 1) = (\\frac{1}{2}, \\frac{1}{2}) \\notin S$。\n\n> [!question] 空間が飛び飛びになる，非連続な空間になる","x":1823,"y":6102,"width":500,"height":292},
		{"id":"da2fda8a2170e483","type":"text","text":"### Subspace\n> [!note] Definition: Subspace\n> A **subspace** of $\\mathbb{R}^n$ is any collection $S$ of vectors in $\\mathbb{R}^n$ such that:\n> \n> 1. The zero vector $\\mathbf{0}$ is in $S$.\n> \n> 2. If $\\mathbf{u}$ and $\\mathbf{v}$ are in $S$, then $\\mathbf{u} + \\mathbf{v}$ is in $S$.  \n>    *(S is closed under addition.)*\n> \n> 3. If $\\mathbf{u}$ is in $S$ and $c$ is a scalar, then $c\\mathbf{u}$ is in $S$.  \n>    *(S is closed under scalar multiplication.)*\n","x":-213,"y":6921,"width":596,"height":440},
		{"id":"a46da33bb3ea2c96","type":"text","text":"> [!tip] Theorem 3.19\n> Let $\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k$ be vectors in $\\mathbb{R}^n$.\n>  Then $\\text{span}(\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k)$ is a subspace of $\\mathbb{R}^n$.\n\n「そりゃそうだろ，vectorで表現できてるならそれは$\\mathbb{R}^n$の外には届かないだろ」ってなった．","x":-213,"y":7687,"width":422,"height":240},
		{"id":"ab5b6a61adc38fb3","type":"text","text":"> [!success] 自分の言葉で言い直した\n> - In $\\mathbb{R}^n$ において、$\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\dots, \\mathbf{v}_k$ からなる $\\text{span}(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\dots, \\mathbf{v}_k)$ は $\\mathbb{R}^n$ のサブスペースである。\n> \n> さらに,\n> \n> - $\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\dots, \\mathbf{v}_k$ が **線形独立** である場合：\n>   - $\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\dots, \\mathbf{v}_k$ はこのサブスペースの基底となる。\n>   - 基底である条件：\n>     1. 線形独立性を持つ。\n>     2. サブスペース全体を張る（Spanする）。\n>   - このとき、$\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\dots, \\mathbf{v}_k$ の数 $k$ はサブスペースの次元 $\\dim(S)$ に等しい。\n> ","x":22,"y":8323,"width":800,"height":360},
		{"id":"de29fa9c9e499c5b","type":"text","text":"> [!cite] Proof\n> Let $S = \\text{span}(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k)$. To check **property (1)** of the definition, we simply observe that the zero vector $\\mathbf{0}$ is in $S$, since:\n> \n> $$\n> \\mathbf{0} = 0 \\mathbf{v}_1 + 0 \\mathbf{v}_2 + \\cdots + 0 \\mathbf{v}_k.\n> $$\n> \n> Now let:\n> \n> $$\n> \\mathbf{u} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k\n> $$\n> \n> and\n> \n> $$\n> \\mathbf{v} = d_1 \\mathbf{v}_1 + d_2 \\mathbf{v}_2 + \\cdots + d_k \\mathbf{v}_k\n> $$\n> \n> be two vectors in $S$. Then:\n> \n> $$\n> \\mathbf{u} + \\mathbf{v} = (c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k) + (d_1 \\mathbf{v}_1 + d_2 \\mathbf{v}_2 + \\cdots + d_k \\mathbf{v}_k)\n> $$\n> \n> $$\n> = (c_1 + d_1) \\mathbf{v}_1 + (c_2 + d_2) \\mathbf{v}_2 + \\cdots + (c_k + d_k) \\mathbf{v}_k.\n> $$\n> \n> Thus, $\\mathbf{u} + \\mathbf{v}$ is a linear combination of $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k$ and so is in $S$. This verifies **property (2)**.\n> \n> To show **property (3)**, let $c$ be a scalar. Then:\n> \n> $$\n> c \\mathbf{u} = c (c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k)\n> $$\n> \n> $$\n> = (c c_1) \\mathbf{v}_1 + (c c_2) \\mathbf{v}_2 + \\cdots + (c c_k) \\mathbf{v}_k,\n> $$\n> \n> which shows that $c \\mathbf{u}$ is also a linear combination of $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k$ and is therefore in $S$. We have shown that $S$ satisfies **properties (1), (2), and (3)** and hence is a subspace of $\\mathbb{R}^n$.\n","x":-314,"y":8815,"width":550,"height":730},
		{"id":"db2670541536258e","type":"text","text":"## 3.1 Matrix Operations\n### 計算方法\n","x":-292,"y":3460,"width":303,"height":120},
		{"id":"84d9da5465d43d7f","type":"text","text":"> [!tip] Theorem 3.6\n> ####  Uniqueness of the Inverse\n> If $A$ is an invertible matrix, then its inverse is unique.\n","x":4426,"y":3460,"width":475,"height":170},
		{"id":"a777fa6f38d3d9de","type":"text","text":"## 3.3 The Inverse of a Matrix","x":3780,"y":3460,"width":362,"height":56},
		{"id":"ac93ec546cb9e8af","type":"text","text":"### Linear Transformations\n\n> [!note] Definition\n> A transformation $T : \\mathbb{R}^n \\to \\mathbb{R}^m$ is called a **linear transformation** if:\n> \n> 1. $T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$ for all $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$.\n> \n> 2. $T(c\\mathbf{v}) = cT(\\mathbf{v})$ for all $\\mathbf{v}$ in $\\mathbb{R}^n$ and all scalars $c$.\n> \n\n> [!warning] Notes\n> They map vectors from one space to another while maintaining the structure of the vector space.\n> ","x":-254,"y":10240,"width":608,"height":406},
		{"id":"0d68b779dd60a842","type":"text","text":"> [!example] Example 3.55\n> \n> **Problem**  \n> Consikder once again the transformation $T : \\mathbb{R}^2 \\to \\mathbb{R}^3$ defined by:\n> \n> $$\n> T\\begin{bmatrix}x \\\\ y\\end{bmatrix} = \n> \\begin{bmatrix}\n> x \\\\ \n> 2x - y \\\\ \n> 3x + 4y\n> \\end{bmatrix}.\n> $$\n> \n> **Solution**  \n> Let’s check that $T$ is a linear transformation. To verify:\n> \n> > [!Todo] **Addition property:**  \n> >    Let $\\mathbf{u} = \\begin{bmatrix}u_1 \\\\ u_2\\end{bmatrix}$ and $\\mathbf{v} = \\begin{bmatrix}v_1 \\\\ v_2\\end{bmatrix}$. Then:\n> > \n> >    $$\n> >    T(\\mathbf{u} + \\mathbf{v}) = T\\begin{bmatrix}u_1 + v_1 \\\\ u_2 + v_2\\end{bmatrix} = \n> >    \\begin{bmatrix}\n> >    u_1 + v_1 \\\\\n> >    2(u_1 + v_1) - (u_2 + v_2) \\\\\n> >    3(u_1 + v_1) + 4(u_2 + v_2)\n> >    \\end{bmatrix}.\n> >    $$\n> > \n> >    By distributing terms, this is equivalent to:\n> > \n> >    $$\n> >    T(\\mathbf{u}) + T(\\mathbf{v}) = \n> >    \\begin{bmatrix}\n> >    u_1 \\\\\n> >    2u_1 - u_2 \\\\\n> >    3u_1 + 4u_2\n> >    \\end{bmatrix}\n> >    +\n> >    \\begin{bmatrix}\n> >    v_1 \\\\\n> >    2v_1 - v_2 \\\\\n> >    3v_1 + 4v_2\n> >    \\end{bmatrix}.\n> >    $$\n> > \n> >    Hence, the addition property is satisfied.\n>  \n> > [!Todo] **Scalar multiplication property:**  \n> >    Let $c$ be a scalar and $\\mathbf{u} = \\begin{bmatrix}u_1 \\\\ u_2\\end{bmatrix}$. Then:\n> > \n> >    $$\n> >    T(c\\mathbf{u}) = T\\begin{bmatrix}cu_1 \\\\ cu_2\\end{bmatrix} = \n> >    \\begin{bmatrix}\n> >    cu_1 \\\\\n> >    2(cu_1) - (cu_2) \\\\\n> >    3(cu_1) + 4(cu_2)\n> >    \\end{bmatrix}.\n> >    $$\n> > \n> >    By factoring out $c$, this is equivalent to:\n> > \n> >    $$\n> >    cT(\\mathbf{u}) = c \\begin{bmatrix}\n> >    u_1 \\\\\n> >    2u_1 - u_2 \\\\\n> >    3u_1 + 4u_2\n> >    \\end{bmatrix}.\n> >    $$\n> > \n> >    Hence, the scalar multiplication property is satisfied.\n > \n>\n> Since both properties are satisfied, $T$ is a linear transformation.","x":460,"y":10240,"width":560,"height":1080},
		{"id":"408e5b9d635922c5","type":"text","text":"A transformation $T : \\mathbb{R}^n \\to \\mathbb{R}^m$ (**n > m**)　の場合\n","x":460,"y":11600,"width":402,"height":50},
		{"id":"63a16e0c49248943","type":"text","text":"A transformation $T : \\mathbb{R}^n \\to \\mathbb{R}^m$ (**n < m**)　の場合\n","x":460,"y":11760,"width":402,"height":50},
		{"id":"7e9bf228246f0519","type":"text","text":"A transformation $T : \\mathbb{R}^n \\to \\mathbb{R}^m$ (**n = m**)　の場合\n","x":862,"y":12080,"width":402,"height":50},
		{"id":"d885407eac2e438f","type":"text","text":"> [!tip]\n> ### Property of Matrix Addition\n> > [!warning] 加算は同じサイズである必要がある\n> \n> Let $A$, $B$, and $C$ be matrices of the same size:\n> \n> a. $A + B = B + A$ (Commutativity)\n> \n> b. $(A + B) + C = A + (B + C)$ (Associativity)\n> \n> c. $A + O = A$\n> \n> d. $A + (-A) = O$\n","x":2234,"y":3583,"width":440,"height":420},
		{"id":"939ae90b31e90842","type":"text","text":"> [!tip]\n> ### Property of Scalar Multiplication\n> Let $A$ be a matrix and $c$, $d$ be scalars:\n> \n> e. $c(A + B) = cA + cB$ (Distributivity)\n> \n> f. $(c + d)A = cA + dA$ (Distributivity)\n> \n> g. $c(dA) = (cd)A$\n> \n> h. $1A = A$","x":2688,"y":3587,"width":411,"height":376},
		{"id":"5380508dd61fdeda","type":"text","text":"> [!tip] Theorem 3.5: Properties of Symmetric Matrices\n> a. If $A$ is a square matrix, then $A + A^T$ is a symmetric matrix.\n> \n> b. For any matrix $A$, $AA^T$ and $A^T A$ are symmetric matrices.\n","x":3134,"y":4142,"width":535,"height":181},
		{"id":"78adc471de7b3e1d","type":"text","text":"> [!tip] Theorem 3.12: The Fundamental Theorem of Invertible Matrices (Version 1)\n> Let $A$ be an $n \\times n$ matrix. The following statements are equivalent:\n> \n> a. $A$ is invertible.  \n> b. $A\\mathbf{x} = \\mathbf{b}$ has a unique solution for every $\\mathbf{b}$ in $\\mathbb{R}^n$.  \n> c. $A\\mathbf{x} = \\mathbf{0}$ has only the trivial solution.  \n> d. The reduced row echelon form of $A$ is $I_n$.  \n> e. $A$ is a product of elementary matrices.\n\n","x":6874,"y":4671,"width":594,"height":340},
		{"id":"d167bb3954894f7c","type":"text","text":"> [!tip] Theorem 3.13\n> Let $A$ be a square matrix.\n> If $B$ is a square matrix such that either $AB = I$ or $BA = I$,\n> then $A$ is invertible and $B = A^{-1}$.","x":6874,"y":5051,"width":511,"height":160},
		{"id":"c6332f8a008274e1","type":"text","text":"> [!tip] Theorem 3.14\n> Let $A$ be a square matrix. If a sequence of elementary row operations reduces $A$ to $I$, then the same sequence of elementary row operations transforms $I$ into $A^{-1}$.","x":6874,"y":5251,"width":498,"height":185},
		{"id":"69cd7a2fdf06ac53","type":"text","text":"### The Fundamental Theorem of Invertible\n\n> [!question] それはInvertibleなんけ？","x":6106,"y":4650,"width":476,"height":141},
		{"id":"7dc2e6c4f186b230","type":"text","text":"### ゆる証明\n\n**(a) ⇒ (b)**\n- **概要**：行列 $A$ が可逆である場合、任意の方程式 $A\\mathbf{x} = \\mathbf{b}$ は一意解を持つ。\n- **理由**：逆行列 $A^{-1}$ を用いることで、解は次のように直接求められる：\n$$\n\\mathbf{x} = A^{-1}\\mathbf{b}.\n$$\n\n---\n\n**(b) ⇒ (c)**\n- **概要**：任意の $A\\mathbf{x} = \\mathbf{b}$ が一意解を持つ場合、特に $A\\mathbf{x} = \\mathbf{0}$ の解が自明（$\\mathbf{x} = \\mathbf{0}$ のみ）である。\n- **理由**：斉次方程式 $A\\mathbf{x} = \\mathbf{0}$ は解として必ず $\\mathbf{x} = \\mathbf{0}$ を含むが、これが唯一の解である。\n\n---\n\n**(c) ⇒ (d)**\n- **概要**：$A\\mathbf{x} = \\mathbf{0}$ が自明解（$\\mathbf{x} = 0$）のみを持つ場合、行列 $A$ の行簡約形は単位行列 $I_n$ になる。\n- **理由**：$A$ の行列ランクがフルランクであることを示しているから：\n$$\n[A | \\mathbf{0}] \\to [I_n | \\mathbf{0}].\n$$\n\n---\n\n**(d) ⇒ (e)**\n- **概要**：行列 $A$ の行簡約形が単位行列 $I_n$ ならば、$A$ はエレメンタリ行列の積として表現できる。\n- **理由**：行基本操作を繰り返す過程はすべてエレメンタリ行列の積として記述可能：\n$$\nE_k \\cdots E_2 E_1 A = I_n \\implies A = E_1^{-1} E_2^{-1} \\cdots E_k^{-1}.\n$$\n\n---\n\n**(e) ⇒ (a)**\n- **概要**：行列 $A$ がエレメンタリ行列の積で表される場合、$A$ は可逆である。\n- **理由**：エレメンタリ行列はすべて可逆であり、その積も可逆になる：\n$$\nA = E_1^{-1} E_2^{-1} \\cdots E_k^{-1}.\n$$\n","x":7682,"y":4511,"width":740,"height":1000},
		{"id":"07a436f13fd511ad","type":"text","text":"### Inverse\n\n> [!note] Definition: Inverse of a Matrix\n> If $A$ is an $n \\times n$ matrix, an **inverse** of $A$ is an $n \\times n$ matrix $A'$ with the property that:\n> \n> $$\n> AA' = I\n> $$\n> \n> and\n> \n> $$\n> A'A = I\n> $$\n> \n> where $I = I_n$ is the $n \\times n$ identity matrix. If such an $A'$ exists, then $A$ is called **invertible**.\n","x":3780,"y":3545,"width":508,"height":379},
		{"id":"6c78954834f4b9ee","type":"text","text":"> [!tip] Theorem 3.9:\n> ### Properties of Inverses\n> \n> a. If $A$ is an invertible matrix, then $A^{-1}$ is invertible and:\n> \n> $$\n> (A^{-1})^{-1} = A.\n> $$\n> \n> b. If $A$ is an invertible matrix and $c$ is a nonzero scalar, then $cA$ is an invertible matrix and:\n> \n> $$\n> (cA)^{-1} = \\frac{1}{c} A^{-1}.\n> $$\n> \n> > [!warning] c, d, e  が線型代数っぽい\n> \n> c. If $A$ and $B$ are invertible matrices of the same size, then $AB$ is invertible and:\n> \n> $$\n> (AB)^{-1} = B^{-1}A^{-1}.\n> $$\n> \n> d. If $A$ is an invertible matrix, then $A^T$ is invertible and:\n> \n> $$\n> (A^T)^{-1} = (A^{-1})^T.\n> $$\n> \n> e. If $A$ is an invertible matrix, then $A^n$ is invertible for all nonnegative integers $n$ and:\n> \n> $$\n> (A^n)^{-1} = (A^{-1})^n.\n> $$\n","x":3846,"y":4483,"width":710,"height":617},
		{"id":"cea6515eb2eaa635","type":"text","text":"> [!note] Definition: \n> ### Negative Power of a Matrix\n> If $A$ is an invertible matrix and $n$ is a positive integer, then $A^{-n}$ is defined by:\n> \n> $$\n> A^{-n} = (A^{-1})^n = (A^n)^{-1}.\n> $$\n","x":3840,"y":5240,"width":448,"height":252},
		{"id":"c63798303434ef2f","type":"text","text":"##### 基本のキ！　部分空間の基底を求める\n> [!example] Example 3.44\n> \n> Problem\n> Find a basis for $S = \\text{span}(\\mathbf{u}, \\mathbf{v}, \\mathbf{w})$, where:\n> $$\n> \\mathbf{u} =\n> \\begin{bmatrix}\n> 3 \\\\\n> -1 \\\\\n> 5\n> \\end{bmatrix},\n> \\mathbf{v} =\n> \\begin{bmatrix}\n> 2 \\\\\n> 1 \\\\\n> 3\n> \\end{bmatrix},\n> \\text{and }\n> \\mathbf{w} =\n> \\begin{bmatrix}\n> 0 \\\\\n> -5 \\\\\n> 1\n> \\end{bmatrix}.\n> $$\n> \n> Solution\n> To find a basis for $S$, we need to determine which of $\\mathbf{u}, \\mathbf{v}, \\mathbf{w}$ are linearly independent. We perform row reduction on the matrix:\n> $$\n> A =\n> \\begin{bmatrix}\n> 3 & 2 & 0 \\\\\n> -1 & 1 & -5 \\\\\n> 5 & 3 & 1\n> \\end{bmatrix}.\n> $$\n> \n> After performing row reduction, we find that:\n> $$\n> A' =\n> \\begin{bmatrix}\n> 1 & 0 & 0 \\\\\n> 0 & 1 & 0 \\\\\n> 0 & 0 & 0\n> \\end{bmatrix}.\n> $$\n> \n> This shows that only $\\mathbf{u}$ and $\\mathbf{v}$ are linearly independent, while $\\mathbf{w}$ is a linear combination of $\\mathbf{u}$ and $\\mathbf{v}$.\n> \n> Thus, the basis for $S$ is:\n> $$\n> \\{\\mathbf{u}, \\mathbf{v}\\}.\n> $$\n> \n\n > [!info]\n > wがu, vで表現できる（線形従属）なので線型独立であるu,vのみがbasisになる","x":3446,"y":7298,"width":680,"height":826},
		{"id":"b4568121d8c03310","type":"text","text":"### Rank\n> [!note] Definition\n> \n> The **rank** of a matrix $A$ is the dimension of its row and column spaces\n> and is denoted by $$rank(A)$$\n\n> [!cite] コメント\n> 行列Aのrow(A)のベクトルの数，もしくはcolumn(A)のベクトルの数\n> 基底の数を数える","x":7146,"y":6749,"width":598,"height":380},
		{"id":"8b124a8252aeb0bc","type":"text","text":"### Dimension\n> [!note] Definition: Dimension of a Subspace\n> If $S$ is a subspace of $\\mathbb{R}^n$, \n> then the number of vectors in a basis for $S$ is called the **dimension** of $S$, \n> denoted $$dim\\ (S)$$\n\n> [!cite] コメント\n> 行列が作用する空間全体の話\nお## Example: Rank and Dimension Difference\n> \n> #### Example: Rank and Dimension Difference\n> \n> ###### Problem\n> Define the matrix $A$ as follows:\n> $$\n> A =\n> \\begin{bmatrix}\n> 1 & 2 & 3 \\\\\n> 2 & 4 & 6 \\\\\n> 3 & 6 & 9\n> \\end{bmatrix}.\n> $$\n> \n> ###### Column Space and Rank\n> 1. The column vectors of $A$ are:\n>    $$\n>    \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad\n>    \\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}, \\quad\n>    \\mathbf{v}_3 = \\begin{bmatrix} 3 \\\\ 6 \\\\ 9 \\end{bmatrix}.\n>    $$\n> 2. Clearly, $\\mathbf{v}_2$ and $\\mathbf{v}_3$ are scalar multiples of $\\mathbf{v}_1$, so they are **linearly dependent**.\n>    - The dimension of the column space (or the row space) is 1.\n>    - Therefore, $\\text{rank}(A) = 1$.\n> \n> ###### Dimension and Rank\n> - **Dimension:** The matrix acts on $\\mathbb{R}^3$, so the space has a dimension of 3.\n> - **Rank:** The rank of $A$ is 1, since there is only 1 linearly independent column vector.\n> \n> ###### Interpretation\n> This example demonstrates that the rank of a matrix can be smaller than the dimension of the space it acts on when the columns (or rows) are linearly dependent.\n> ","x":6466,"y":6749,"width":634,"height":791},
		{"id":"97aead549af006c6","type":"text","text":"> [!tip] Theorem 3.26: The Rank Theorem\n> \n> If $A$ is an $m \\times n$ matrix, then:\n> \n> $$\n> \\text{rank}(A) + \\text{nullity}(A) = n\n> $$\n","x":6965,"y":7480,"width":433,"height":173},
		{"id":"5342d19226984252","type":"text","text":"> [!tip] Theorem 3.24\n> \n> The row and column spaces of a matrix $A$ have the same dimension.\n> $$\\dim(\\text{row}(A)) = \\dim(\\text{col}(A))$$ ","x":6519,"y":7599,"width":446,"height":200},
		{"id":"b4e3aa9afe4a736c","type":"text","text":"1. 行列 $A$ の右に単位行列 $I$ を付けてargumented 行列 $[A | I]$ を作る。\n2. 行基本操作を用いて $A$ を $I$ に変換する。\n3. 増強行列の右側が逆行列 $A^{-1}$ になる：\n$$ [A | I] \\to [I | A^{-1}] $$","x":7242,"y":5451,"width":418,"height":177},
		{"id":"491977904fc4fe14","type":"text","text":"> [!tip] Theorem 3.10: Elementary Matrix and Row Operations\n> Let $E$ be the elementary matrix obtained by performing an elementary row operation on $I_n$. If the same elementary row operation is performed on an $n \\times r$ matrix $A$, the result is the same as the matrix $EA$.\n","x":7286,"y":3620,"width":563,"height":197},
		{"id":"92e56ea91c672fee","type":"text","text":"### Elementary Matrices\n\n> [!note] Definition: Elementary Matrix\n> An **elementary matrix** is any matrix that can be obtained by performing an elementary row operation on an identity matrix.\n\n> [!example] Example: Elementary Matrices and Their Actions\n> Let:\n> \n> $$\n> E_1 = \n> \\begin{bmatrix}\n> 1 & 0 & 0 & 0 \\\\\n> 0 & 3 & 0 & 0 \\\\\n> 0 & 0 & 1 & 0 \\\\\n> 0 & 0 & 0 & 1\n> \\end{bmatrix}, \\quad\n> E_2 = \n> \\begin{bmatrix}\n> 0 & 0 & 1 & 0 \\\\\n> 0 & 1 & 0 & 0 \\\\\n> 1 & 0 & 0 & 0 \\\\\n> 0 & 0 & 0 & 1\n> \\end{bmatrix}, \\quad \\text{and} \\quad\n> E_3 = \n> \\begin{bmatrix}\n> 1 & 0 & 0 & 0 \\\\\n> 0 & 1 & 0 & 0 \\\\\n> 0 & 0 & 1 & 0 \\\\\n> 0 & -2 & 0 & 1\n> \\end{bmatrix}.\n> $$\n> \n> Each of these matrices has been obtained from the identity matrix $I_4$ by applying a single elementary row operation.\n> The matrix $E_1$ corresponds to $3R_2$, \n> $E_2$ to $R_1 \\leftrightarrow R_3$,\n> and $E_3$ to $R_4 - 2R_2$. \n> Observe that when we left-multiply a $4 \\times n$ matrix by one of these elementary matrices, the corresponding elementary row operation is performed on the matrix.\n> \n> For example, if:\n> \n> $$\n> A = \n> \\begin{bmatrix}\n> a_{11} & a_{12} & a_{13} \\\\\n> a_{21} & a_{22} & a_{23} \\\\\n> a_{31} & a_{32} & a_{33} \\\\\n> a_{41} & a_{42} & a_{43}\n> \\end{bmatrix},\n> $$\n> \n> then:\n> \n> $$\n> E_1A = \n> \\begin{bmatrix}\n> a_{11} & a_{12} & a_{13} \\\\\n> 3a_{21} & 3a_{22} & 3a_{23} \\\\\n> a_{31} & a_{32} & a_{33} \\\\\n> a_{41} & a_{42} & a_{43}\n> \\end{bmatrix}, \\quad\n> E_2A = \n> \\begin{bmatrix}\n> a_{31} & a_{32} & a_{33} \\\\\n> a_{21} & a_{22} & a_{23} \\\\\n> a_{11} & a_{12} & a_{13} \\\\\n> a_{41} & a_{42} & a_{43}\n> \\end{bmatrix}, \\quad \\text{and} \\quad\n> E_3A = \n> \\begin{bmatrix}\n> a_{11} & a_{12} & a_{13} \\\\\n> a_{21} & a_{22} & a_{23} \\\\\n> a_{31} & a_{32} & a_{33} \\\\\n> a_{41} - 2a_{21} & a_{42} - 2a_{22} & a_{43} - 2a_{23}\n> \\end{bmatrix}.\n> $$\n","x":6106,"y":3488,"width":943,"height":873},
		{"id":"cb9d20df62840edf","type":"text","text":"##### row space の規定を求める\n> [!example] Example 3.45\n> \n> Problem\n> Find a basis for the **row space** of the matrix $A$, where:\n> $$\n> A =\n> \\begin{bmatrix}\n> 1 & 1 & 3 & 1 & 6 \\\\\n> 2 & -1 & 0 & 0 & -1 \\\\\n> -3 & 2 & 1 & -2 & 1 \\\\\n> 4 & 1 & 6 & 1 & 3\n> \\end{bmatrix}.\n> $$\n> \n> Solution\n> To find a basis for the row space of $A$, we compute the reduced row echelon form (RREF) of $A$. The RREF of $A$ is:\n> $$\n> R =\n> \\begin{bmatrix}\n> 1 & 0 & 1 & 0 & -1 \\\\\n> 0 & 1 & 2 & 0 & 3 \\\\\n> 0 & 0 & 0 & 1 & 4 \\\\\n> 0 & 0 & 0 & 0 & 0\n> \\end{bmatrix}.\n> $$\n> \n> The nonzero rows of $R$ form a basis for the row space of $A$. Therefore, the basis for the row space of $A$ is:\n> $$\n> \\left\\{\n> \\begin{bmatrix}\n> 1 & 0 & 1 & 0 & -1\n> \\end{bmatrix},\n> \\begin{bmatrix}\n> 0 & 1 & 2 & 0 & 3\n> \\end{bmatrix},\n> \\begin{bmatrix}\n> 0 & 0 & 0 & 1 & 4\n> \\end{bmatrix}\n> \\right\\}.\n> $$\n> \n\n> [!info]\n> row spaceの場合，Theorem 3.20 のように行き着く先が同じなら\n> row spaceは変換後も同じだし，\n> その基底も一致する．\n> そのためRからそのまま持って来れる","x":4146,"y":7298,"width":680,"height":801},
		{"id":"138714c7de321c70","type":"text","text":"##### col space の基底を求める\n> [!example] Example 3.47\n> \n> Problem\n> Find a basis for the **column space** of the matrix $A$, where:\n> $$\n> A =\n> \\begin{bmatrix}\n> 1 & 1 & 3 & 1 & 6 \\\\\n> 2 & -1 & 0 & 0 & -1 \\\\\n> -3 & 2 & 1 & -2 & 1 \\\\\n> 4 & 1 & 6 & 1 & 3\n> \\end{bmatrix}.\n> $$\n> \n> Solution\n> To find a basis for the column space of $A$, we compute the reduced row echelon form (RREF) of $A$. The RREF of $A$ is:\n> $$\n> R =\n> \\begin{bmatrix}\n> 1 & 0 & 1 & 0 & -1 \\\\\n> 0 & 1 & 2 & 0 & 3 \\\\\n> 0 & 0 & 0 & 1 & 4 \\\\\n> 0 & 0 & 0 & 0 & 0\n> \\end{bmatrix}.\n> $$\n> \n> The pivot columns of $R$ correspond to the pivot columns of the original matrix $A$. Thus, the columns of $A$ that form the basis for the column space are:\n> $$\n> \\begin{bmatrix}\n> 1 \\\\\n> 2 \\\\\n> -3 \\\\\n> 4\n> \\end{bmatrix},\n> \\begin{bmatrix}\n> 1 \\\\\n> -1 \\\\\n> 2 \\\\\n> 1\n> \\end{bmatrix},\n> \\begin{bmatrix}\n> 3 \\\\\n> 0 \\\\\n> 1 \\\\\n> 6\n> \\end{bmatrix}.\n> $$\n> \n> Therefore, the basis for the column space of $A$ is:\n> $$\n> \\left\\{\n> \\begin{bmatrix}\n> 1 \\\\\n> 2 \\\\\n> -3 \\\\\n> 4\n> \\end{bmatrix},\n> \\begin{bmatrix}\n> 1 \\\\\n> -1 \\\\\n> 2 \\\\\n> 1\n> \\end{bmatrix},\n> \\begin{bmatrix}\n> 3 \\\\\n> 0 \\\\\n> 1 \\\\\n> 6\n> \\end{bmatrix}\n> \\right\\}.\n> $$\n> ","x":4846,"y":7298,"width":680,"height":801},
		{"id":"408ea514ecdd2a0b","type":"text","text":"### Basis\n\n> [!note] Definition\n>\n> A **basis** for a subspace $S$ of $\\mathbb{R}^n$ is a set of vectors in $S$ that:\n>\n> 1. **spans** $S$, and\n> > [!cite] basisで空間Sの中であればどこでも表現できる\n> 2. is **linearly independent**.\n> > [!cite] basisはvectorの集合であり，そのvectorsは線型独立である","x":3446,"y":6840,"width":620,"height":418},
		{"id":"ebb6cd7ad5cc169b","type":"text","text":"> [!tip] Theorem 3.25\n> \n> For any matrix $A$,\n> \n> $$\n> \\text{rank}(A^T) = \\text{rank}(A).\n> $$\n> ","x":7482,"y":7169,"width":459,"height":163},
		{"id":"45f57a5e5c4b982e","type":"text","text":"### Nullity\n> [!note] Definition\n> The **nullity** of a matrix $A$ is the dimension of its null space and is denoted by $\\text{nullity}(A)$.\n> ","x":7786,"y":6749,"width":540,"height":220},
		{"id":"78d29895fb0aba15","type":"text","text":"> [!note] Theorem 3.27: The Fundamental Theorem of Invertible Matrices (Version 2)\n> \n> Let $A$ be an $n \\times n$ matrix. The following statements are equivalent:\n> \n> > [!cite] from Ch 3.3 Inverse\n> > 1. $A$ is invertible.\n> > 2. The linear system $Ax = b$ has a unique solution for every $b \\in \\mathbb{R}^n$.\n> > 3. The homogeneous equation $Ax = 0$ has only the trivial solution.\n> > 4. The reduced row echelon form of $A$ is the identity matrix $I_n$.\n> > 5. $A$ is a product of elementary matrices.\n> \n> > [!success] from Ch 3.5 Dimension and Rank\n> > 6. $\\text{rank}(A) = n$.\n> > 7. $\\text{nullity}(A) = 0$.\n> > 8. The column vectors of $A$ are linearly independent.\n> > 9. The column vectors of $A$ span $\\mathbb{R}^n$.\n> > 10. The column vectors of $A$ form a basis for $\\mathbb{R}^n$.\n> > 11. The row vectors of $A$ are linearly independent.\n> > 12. The row vectors of $A$ span $\\mathbb{R}^n$.\n> > 13. The row vectors of $A$ form a basis for $\\mathbb{R}^n$\n> \n\n> [!cite] comment\n> **正方形に限る**","x":7711,"y":7540,"width":691,"height":800},
		{"id":"b17bc95648034f23","type":"text","text":"> [!tip] Theorem 3.11: Invertibility of Elementary Matrices\n> Each elementary matrix is invertible, and its inverse is an elementary matrix of the same type.\n\n","x":7286,"y":3886,"width":563,"height":130},
		{"id":"1b4cd495a61a4ea7","type":"text","text":"##### Proof\n命題 a:\n$$\n\\|\\mathbf{v}\\| = 0 \\iff \\mathbf{v} = \\mathbf{0}\n$$\n\n- **必要条件 $(\\Rightarrow)$**:  \n\n  ノルムの定義を使い、$\\|\\mathbf{v}\\| = 0$ のとき $\\mathbf{v} \\cdot \\mathbf{v} = 0$ であることを示す。  \n  内積がゼロになるのは $\\mathbf{v}$ がゼロベクトルの場合のみ。\n\n- **十分条件 $(\\Leftarrow)$**:  \n  $\\mathbf{v} = \\mathbf{0}$ であれば、ノルムの定義より $\\|\\mathbf{v}\\| = 0$ が成立する。\n\n---\n\n命題 b:\n$$\n\\|c\\mathbf{v}\\| = |c| \\|\\mathbf{v}\\|\n$$\n\n- ノルムの定義を使う：\n$$\n\\|c\\mathbf{v}\\| = \\sqrt{(c\\mathbf{v}) \\cdot (c\\mathbf{v})}.\n$$\n- 内積のスカラー倍の性質を適用し、\n$$\n(c\\mathbf{v}) \\cdot (c\\mathbf{v}) = c^2 (\\mathbf{v} \\cdot \\mathbf{v})\n$$\nを確認する。\n- $\\sqrt{c^2} = |c|$ を使い、最終的にノルムの式を得る。\n","x":-2173,"y":3865,"width":545,"height":660},
		{"id":"94a56f5d160f8d6a","type":"text","text":"##### Proof\n方針：それぞれのベクトルをコンポーネントで表現して，左辺から右辺，もしくはその反対が成り立つかを証明する．","x":-2150,"y":3600,"width":250,"height":230},
		{"id":"2584069adee084f0","type":"text","text":"### The Dot Product\n> [!error] Dot Product\n> If\n> $$\n> \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix}, \\quad\n> \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix},\n> $$\n> then the **dot product** is defined as\n> $$\n> \\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n.\n> $$\n> ","x":-3495,"y":3535,"width":437,"height":370},
		{"id":"2533422c74c97d56","type":"text","text":"> [!important] Theorem 1.2\n> Let $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{w}$ be vectors in $\\mathbb{R}^n$, and let $c$ be a scalar. Then:\n> \n> 1. **Commutativity**  \n>    $$\n>    \\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u}\n>    $$\n> \n> 2. **Distributivity**  \n>    $$\n>    \\mathbf{u} \\cdot (\\mathbf{v} + \\mathbf{w}) = \\mathbf{u} \\cdot \\mathbf{v} + \\mathbf{u} \\cdot \\mathbf{w}\n>    $$\n> \n> 3. **Scalar Multiplication**  \n>    $$\n>    (c\\mathbf{u}) \\cdot \\mathbf{v} = c(\\mathbf{u} \\cdot \\mathbf{v})\n>    $$\n> \n> 4. **Non-Negativity and Zero Condition**  \n>    $$\n>    \\mathbf{u} \\cdot \\mathbf{u} \\geq 0, \\quad \\mathbf{u} \\cdot \\mathbf{u} = 0 \\text{ if and only if } \\mathbf{u} = \\mathbf{0}.\n>    $$\n","x":-2873,"y":3485,"width":549,"height":460},
		{"id":"696e413b6a7aee6c","type":"text","text":"### Projection\n\nConsider two nonzero vectors $\\mathbf{u}$ and $\\mathbf{v}$. Let $\\mathbf{p}$ be the vector obtained by dropping a perpendicular from the head of $\\mathbf{v}$ onto $\\mathbf{u}$, and let $\\theta$ be the angle between $\\mathbf{u}$ and $\\mathbf{v}$.\n\n![[projection.png]]\n\nThen clearly:\n\n$$\n\\mathbf{p} = \\|\\mathbf{p}\\| \\hat{\\mathbf{u}},\n$$\n\nwhere $\\hat{\\mathbf{u}} = \\frac{1}{\\|\\mathbf{u}\\|}\\mathbf{u}$ is the unit vector in the direction of $\\mathbf{u}$. Moreover, elementary trigonometry gives:\n\n$$\n\\|\\mathbf{p}\\| = \\|\\mathbf{v}\\| \\cos \\theta,\n$$\n\nand we know that:\n\n$$\n\\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}.\n$$\n\nThus, after substitution, we obtain:\n\n$$\n\\mathbf{p} = \\|\\mathbf{v}\\| \n\\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|} \\right)\n\\left( \\frac{1}{\\|\\mathbf{u}\\|} \\right) \\mathbf{u}.\n$$\n\n$$\n= \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|^2} \\right) \\mathbf{u}.\n$$\n\n$$\n= \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{u} \\cdot \\mathbf{u}} \\right) \\mathbf{u}.\n$$\n\n> [!error] Definition\n> If $\\mathbf{u}$ and $\\mathbf{v}$ are vectors in $\\mathbb{R}^n$ and $\\mathbf{u} \\neq \\mathbf{0}$, then the **projection of $\\mathbf{v}$ onto $\\mathbf{u}$** is the vector $\\text{proj}_{\\mathbf{u}}(\\mathbf{v})$ defined by:\n> \n> $$\n> \\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{u} \\cdot \\mathbf{u}} \\right) \\mathbf{u}.\n> $$","x":-1526,"y":3510,"width":800,"height":880},
		{"id":"9b827c5ca9dc8066","type":"text","text":"## 4.1 Intro to Eigenvalues and Eigenvectors\n\n> [!note] Definition: Eigenvalue and Eigenvector\n> Let $A$ be an $n \\times n$ matrix. A scalar $\\lambda$ is called an **eigenvalue** of $A$ if there is a nonzero vector $\\mathbf{x}$ such that:\n> $$\n> A\\mathbf{x} = \\lambda\\mathbf{x}\n> $$\n> Such a vector $\\mathbf{x}$ is called an **eigenvector** of $A$ corresponding to $\\lambda$.","x":9000,"y":3280,"width":560,"height":300},
		{"id":"876ddfe4682aa3d2","type":"text","text":"### Eigenspace\n\n> [!note] Eigenspace\n> Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be an eigenvalue of $A$. The collection of all eigenvectors corresponding to $\\lambda$, together with the zero vector, is called the **eigenspace** of $\\lambda$ and is denoted by $E_\\lambda$.\n> ","x":9000,"y":3620,"width":484,"height":260},
		{"id":"20a028c8a7848aaf","type":"text","text":"> [!important] Theorem 1.4\n> #### The Cauch-schwarz Inequality\n> For all vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, \n>\n> $$\n> |\\mathbf{u} \\cdot \\mathbf{v}| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|.\n> $$\n\n","x":-2817,"y":4295,"width":469,"height":220},
		{"id":"f2faf3e9f5c1ff86","type":"text","text":"> [!important] Theorem 1.6\n> #### Pythagoras' Theorem  (ピタゴラスの定理)\n> For all vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, \n> \n> $$\n> \\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2\n> $$\n> \n> if and only if $\\mathbf{u}$ and $\\mathbf{v}$ are orthogonal.\n","x":-2804,"y":4875,"width":480,"height":255},
		{"id":"3d5b418e6fec6764","type":"text","text":"> [!important] Theorem 1.5\n> #### The Triangle Inequlity\n> For all vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, \n> \n> $$\n> \\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|.\n> $$","x":-2793,"y":4545,"width":421,"height":213},
		{"id":"58911c58cb510c61","type":"text","text":"> [!todo] 理解してない 💩","x":-2473,"y":4485,"width":250,"height":60},
		{"id":"406f7005658e7340","type":"text","text":"> [!important] Theorem 1.3\n> Let $\\mathbf{v}$ be a vector in $\\mathbb{R}^n$ and let $c$ be a scalar. Then:\n> \n> 1.  \n>    $$\n>    \\|\\mathbf{v}\\| = 0 \\iff \\mathbf{v} = \\mathbf{0}\n>    $$\n> \n> 2.  \n>    $$\n>    \\|c\\mathbf{v}\\| = |c| \\|\\mathbf{v}\\|\n>    $$\n> ","x":-2817,"y":3985,"width":469,"height":280},
		{"id":"8f99bae5dbb985e8","type":"text","text":"##### Proof\n$$ \\|\\mathbf{u} + \\mathbf{v}\\| ^2 $$ から始ると証明できる\n","x":-2173,"y":4625,"width":289,"height":145},
		{"id":"a663a8355b44526b","type":"text","text":"### Length\n\n> [!error] Length\n> The **length (or norm)** of a vector \n> \n> $$\n> \\mathbf{v} =\n> \\begin{bmatrix}\n> v_1 \\\\\n> v_2 \\\\\n> \\vdots \\\\\n> v_n\n> \\end{bmatrix}\n> $$\n> \n> in $\\mathbb{R}^n$ is the nonnegative scalar $\\|\\mathbf{v}\\|$ defined by:\n> \n> $$\n> \\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}.\n> $$\n> ","x":-3495,"y":3945,"width":517,"height":380},
		{"id":"78bab4ed6f7caf9d","type":"text","text":"### Distance\n> [!error] Distance\n> The **distance** $d(\\mathbf{u}, \\mathbf{v})$ between vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$ is defined by:\n> \n> $$\n> d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|.\n> $$\n","x":-3526,"y":4355,"width":580,"height":240},
		{"id":"3219b07f43b22a12","type":"text","text":"### Angles\n> [!error] Angles\n> For nonzero vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, \n> \n> $$\n> \\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}.\n> $$\n\n","x":-3502,"y":4625,"width":400,"height":240},
		{"id":"0b42723ecde3a0fc","type":"text","text":"### Orthogonal Vectors\n\n> [!error] Orthogonal Vectors\n> Two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$ are **orthogonal** to each other if:\n> $$\n> \\mathbf{u} \\cdot \\mathbf{v} = 0.\n> $$\n","x":-3524,"y":4892,"width":609,"height":220},
		{"id":"b2a67b5f4d60d2d6","type":"text","text":"## 1.2 Length and Angle: The Dot Product\n## 線と角度","x":-3552,"y":3300,"width":500,"height":100},
		{"id":"8aac3df908ad63c0","type":"text","text":"> [!important] Theorem 1.1 Algebraic Properties of Vector in $\\mathbb{R}^n$\n> Let $\\vec{u}$, $\\vec{v}$, and $\\vec{w}$ be vectors in $\\mathbb{R}^n$\n> Let $c$ and $d$ be scalars.<br>\n> Then:\n> 1. **Commutativity:**\n>    $$\n>    \\vec{u} + \\vec{v} = \\vec{v} + \\vec{u}\n>    $$\n> \n> 2. **Associativity:**\n>    $$\n>    (\\vec{u} + \\vec{v}) + \\vec{w} = \\vec{u} + (\\vec{v} + \\vec{w})\n>    $$\n> \n> 3. **Identity Element:**\n>    $$\n>    \\vec{u} + \\vec{0} = \\vec{u}\n>    $$\n> \n> 4. **Inverse Element:**\n>    $$\n>    \\vec{u} + (-\\vec{u}) = \\vec{0}\n>    $$\n> \n> 5. **Distributivity of Scalars over Vector Addition:**\n>    $$\n>    c(\\vec{u} + \\vec{v}) = c\\vec{u} + c\\vec{v}\n>    $$\n> \n> 6. **Distributivity of Scalars over Scalar Addition:**\n>    $$\n>    (c + d)\\vec{u} = c\\vec{u} + d\\vec{u}\n>    $$\n> \n> 7. **Associativity of Scalar Multiplication:**\n>    $$\n>    c(d\\vec{u}) = (cd)\\vec{u}\n>    $$\n> \n> 8. **Identity Scalar:**\n>    $$\n>    1\\vec{u} = \\vec{u}\n>    $$","x":-4824,"y":3485,"width":619,"height":815},
		{"id":"0f560b2e1a6094fb","type":"text","text":"### Normal Form of a Line  \n> [!caution] $\\mathbb{R}^2$\n> \n> > [!error] Normal Form\n> > The **normal form of the equation of a line** $\\ell$ in $\\mathbb{R}^2$ is:\n> > \n> > $$\n> > \\mathbf{n} \\cdot (\\mathbf{x} - \\mathbf{p}) = 0\n> > $$\n> > \n> > or equivalently:\n> > \n> > $$\n> > \\mathbf{n} \\cdot \\mathbf{x} = \\mathbf{n} \\cdot \\mathbf{p},\n> > $$\n> > \n> > where $\\mathbf{p}$ is a specific point on $\\ell$ and $\\mathbf{n} \\neq \\mathbf{0}$ is a normal vector for $\\ell$.\n> \n> > [!tip] General Form\n> > The **general form of the equation of $\\ell$** is:\n> > \n> > $$\n> > ax + by = c,\n> > $$\n> > \n> > where $\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\end{bmatrix}$ is a normal vector for $\\ell$.\n> ","x":-4304,"y":4920,"width":637,"height":623},
		{"id":"b7078196d6c5aa74","type":"text","text":"### Vector Form of a Line\n> [!caution] $\\mathbb{R}^2$ and $\\mathbb{R}^3$\n> \n> > [!error] Vector Form\n> > The **vector form of the equation of a line** $\\ell$ in $\\mathbb{R}^2$ or $\\mathbb{R}^3$ is:\n> > \n> > $$\n> > \\mathbf{x} = \\mathbf{p} + t \\mathbf{d},\n> > $$\n> > \n> > where $\\mathbf{p}$ is a specific point on $\\ell$ and $\\mathbf{d} \\neq \\mathbf{0}$ is a direction vector for $\\ell$.\n>  \n> > [!tip] Parametric Equations\n> > The equations corresponding to the components of the vector form of the equation are called **parametric equations** of $\\ell$.\n","x":-4304,"y":5564,"width":621,"height":504},
		{"id":"ca65d19aec8bef69","type":"text","text":"### Vector Form of a Plane\n> [!info] Vector Form\n> The **vector form of the equation of a plane** $\\mathcal{P}$ in $\\mathbb{R}^3$ is:\n> \n> $$\n> \\mathbf{x} = \\mathbf{p} + s \\mathbf{u} + t \\mathbf{v},\n> $$\n> \n> where $\\mathbf{p}$ is a point on $\\mathcal{P}$ and $\\mathbf{u}$ and $\\mathbf{v}$ are direction vectors for $\\mathcal{P}$ ( $\\mathbf{u}$ and $\\mathbf{v}$ are nonzero and parallel to $\\mathcal{P}$, but not parallel to each other).\n\n> [!tip] Parametric Equations\n> The equations corresponding to the components of the vector form of the equation are called **parametric equations** of $\\mathcal{P}$.\n","x":-4976,"y":5624,"width":569,"height":438},
		{"id":"7821604b06949d71","type":"text","text":"## Lines in $\\mathbb{R}^2$, $\\mathbb{R}^3$","x":-4865,"y":5345,"width":212,"height":62},
		{"id":"f62f49400e1d88b7","type":"text","text":"### Linear Combinations\n> [!danger] Linear Combinations\n> Let $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k \\in \\mathbb{R}^n$. <br>\n> A vector $\\mathbf{v} \\in \\mathbb{R}^n$ is called  $\\textbf{linear combination}$ <br>\n> if there exists a set of scalars $c_1, c_2, \\ldots, c_k \\in \\mathbb{R}$ such that:\n> $$\n> \\mathbf{v} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k.\n> $$","x":-4809,"y":4370,"width":500,"height":290},
		{"id":"3ff3bf252eab0d04","type":"text","text":"##### Proof\n方針：それぞれのベクトルをコンポーネントで表現して，左辺から右辺，もしくはその反対が成り立つかを証明する．","x":-4031,"y":3778,"width":250,"height":230},
		{"id":"39ae9f27a0b3906d","type":"text","text":"### Vectors in $\\mathbb{R}^n$","x":-5206,"y":4070,"width":200,"height":70},
		{"id":"afe4f6e6d1a8c317","type":"text","text":"## 1.1 The Geometry and Algebra of Vectors","x":-5466,"y":4370,"width":520,"height":70},
		{"id":"4090a8b66c9c27f9","type":"text","text":"## 1.3 Lines and Planes","x":-5286,"y":5145,"width":318,"height":70},
		{"id":"4c2e13bd5a11f092","type":"text","text":"### Normal Form of a Plane\n> [!info] Normal Form\n> The **normal form of the equation of a plane** $\\mathcal{P}$ in $\\mathbb{R}^3$ is:\n> \n> $$\n> \\mathbf{n} \\cdot (\\mathbf{x} - \\mathbf{p}) = 0\n> $$\n> \n> or equivalently:\n> \n> $$\n> \\mathbf{n} \\cdot \\mathbf{x} = \\mathbf{n} \\cdot \\mathbf{p},\n> $$\n> \n> where $\\mathbf{p}$ is a specific point on $\\mathcal{P}$ and $\\mathbf{n} \\neq \\mathbf{0}$ is a normal vector for $\\mathcal{P}$.\n\n> [!tip] General Form\n> The **general form of the equation of $\\mathcal{P}$** is:\n> \n> $$\n> ax + by + cz = d,\n> $$\n> \n> where $\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}$ is a normal vector for $\\mathcal{P}$.\n","x":-5580,"y":5564,"width":569,"height":558},
		{"id":"63f325337c600605","type":"text","text":"## Planes in $\\mathbb{R}^3$","x":-5296,"y":5382,"width":196,"height":50},
		{"id":"117df86d5b950020","type":"text","text":"## 4.2 Determinants\n\n> [!note] Definition\n> Let \n> $$\n> A = \n> \\begin{bmatrix}\n> a_{11} & a_{12} & a_{13} \\\\\n> a_{21} & a_{22} & a_{23} \\\\\n> a_{31} & a_{32} & a_{33}\n> \\end{bmatrix}.\n> $$ \n> Then the **determinant** of $A$ is the scalar:\n> \n> $$\n> \\det(A) = |A| = a_{11}\n> \\begin{vmatrix}\n> a_{22} & a_{23} \\\\\n> a_{32} & a_{33}\n> \\end{vmatrix}\n> - a_{12}\n> \\begin{vmatrix}\n> a_{21} & a_{23} \\\\\n> a_{31} & a_{33}\n> \\end{vmatrix}\n> + a_{13}\n> \\begin{vmatrix}\n> a_{21} & a_{22} \\\\\n> a_{31} & a_{32}\n> \\end{vmatrix}.\n> $$\n> ","x":9600,"y":3260,"width":540,"height":389},
		{"id":"d5c08f0d0a517d7f","type":"text","text":"### Triangular matrix と Eigenvalues\n> [!tip] Theorem 4.15\n> \n> The eigenvalues of a triangular matrix are the entries on its main diagonal.\n\n> [!example] Example 4.20\n> Consider the triangular matrix:\n> \n> $$\n> A = \n> \\begin{bmatrix}\n> 2 & 1 & 3 \\\\\n> 0 & 5 & 7 \\\\\n> 0 & 0 & 4\n> \\end{bmatrix}.\n> $$\n> \n> #### Eigenvalues:\n> The eigenvalues of $A$ are the entries on its main diagonal:\n> \n> $$\n> \\lambda_1 = 2, \\, \\lambda_2 = 5, \\, \\lambda_3 = 4.\n> $$\n> \n> #### Verification:\n> To verify this, note that for triangular matrices, the determinant of $A - \\lambda I$ simplifies to the product of the diagonal entries. For $A$,\n> \n> $$\n> A - \\lambda I = \n> \\begin{bmatrix}\n> 2 - \\lambda & 1 & 3 \\\\\n> 0 & 5 - \\lambda & 7 \\\\\n> 0 & 0 & 4 - \\lambda\n> \\end{bmatrix}.\n> $$\n> \n> The determinant is:\n> \n> $$\n> \\text{det}(A - \\lambda I) = (2 - \\lambda)(5 - \\lambda)(4 - \\lambda).\n> $$\n> \n> Setting $\\text{det}(A - \\lambda I) = 0$, the roots are:\n> \n> $$\n> \\lambda = 2, \\, 5, \\, 4.\n> $$\n> \n> Thus, the eigenvalues are confirmed to be the diagonal entries of $A$.\n> ","x":10180,"y":3398,"width":635,"height":870},
		{"id":"22c919768119f16b","x":10855,"y":3398,"width":926,"height":1479,"type":"text","text":"### Invertible?? or NOT invertible?\n> [!tip] Theorem 4.16\n> \n> A square matrix $A$ is invertible if and only if $0$ is **not** an eigenvalue of $A$.\n\n> [!example] Invertible example since $\\lambda \\neq 0$\n> Let:\n> $$\n> A = \\begin{bmatrix}\n> 1 & 2 \\\\\n> 3 & 4\n> \\end{bmatrix}.\n> $$\n> \n> #### Step 1: Find the eigenvalues of $A$.\n> The eigenvalues are the solutions to the characteristic equation:\n> $$\n> \\det(A - \\lambda I) = 0.\n> $$\n> Compute the determinant:\n> $$\n> \\det\\begin{bmatrix}\n> 1 - \\lambda & 2 \\\\\n> 3 & 4 - \\lambda\n> \\end{bmatrix} = (1 - \\lambda)(4 - \\lambda) - (2)(3) = 0.\n> $$\n> Simplify:\n> $$\n> (1 - \\lambda)(4 - \\lambda) - 6 = 0 \\\\\n> (1 \\cdot 4 - 1 \\cdot \\lambda - \\lambda \\cdot 4 + \\lambda^2) - 6 = 0 \\\\\n> (4 - 1\\lambda - 4\\lambda + \\lambda^2) - 6 = 0 \\\\\n> (4 - 5\\lambda + \\lambda^2) - 6 = 0 \\\\\n> \\lambda^2 - 5\\lambda - 2 = 0.\n> $$\n> Solving the quadratic equation:\n> $$\n> \\lambda = \\frac{5 \\pm \\sqrt{(-5)^2 - 4 \\cdot 1 \\cdot (-2)}}{2} = \\frac{5 \\pm \\sqrt{25 + 8}}{2} = \\frac{5 \\pm \\sqrt{33}}{2}.\n> $$\n> \n> #### Step 2: Determine if $0$ is an eigenvalue.\n> Since neither eigenvalue equals $0$, the matrix $A$ is invertible according to Theorem 4.16.\n\n\n> [!example] **NOT** Invertible example since $\\lambda = 0$\n> ### Counterexample (When $0$ is an eigenvalue)\n> \n> Let:\n> $$\n> B = \\begin{bmatrix}\n> 1 & 0 \\\\\n> 0 & 0\n> \\end{bmatrix}.\n> $$\n> \n> #### Step 1: Find the eigenvalues of $B$.\n> Compute the determinant:\n> $$\n> \\det(B - \\lambda I) = \\det\\begin{bmatrix}\n> 1 - \\lambda & 0 \\\\\n> 0 & -\\lambda\n> \\end{bmatrix} = (1 - \\lambda)(-\\lambda).\n> $$\n> Setting the determinant to zero:\n> $$\n> (1 - \\lambda)(-\\lambda) = 0.\n> $$\n> The eigenvalues are:\n> $$\n> \\lambda = 0, \\, 1.\n> $$\n> \n> #### Step 2: Determine if $0$ is an eigenvalue.\n> Since $0$ is an eigenvalue, the matrix $B$ is **not** invertible, which agrees with Theorem 4.16.\n> \n\nThese examples demonstrate the relationship between eigenvalues and invertibility."},
		{"id":"aa33b901935b7efc","x":10187,"y":3300,"width":628,"height":57,"type":"text","text":"## 4.3 Eigenvalues and Eigenvectors of n x n Matrices"},
		{"id":"3e0bfe748403fe5e","x":11800,"y":3398,"width":686,"height":822,"type":"text","text":"> [!tip] Theorem 4.17: The Fundamental Theorem of Invertible Matrices: Version 3\n> \n> Let $A$ be an $n \\times n$ matrix. The following statements are equivalent:\n> \n> > [!cite] from Ch 3.3 Inverse\n> > 1. $A$ is invertible.\n> > 2. The linear system $Ax = b$ has a unique solution for every $b \\in \\mathbb{R}^n$.\n> > 3. The homogeneous equation $Ax = 0$ has only the trivial solution.\n> > 4. The reduced row echelon form of $A$ is the identity matrix $I_n$.\n> > 5. $A$ is a product of elementary matrices.\n> \n> > [!cite] from Ch 3.5 Dimension and Rank\n> > 6. $\\text{rank}(A) = n$.\n> > 7. $\\text{nullity}(A) = 0$.\n> > 8. The column vectors of $A$ are linearly independent.\n> > 9. The column vectors of $A$ span $\\mathbb{R}^n$.\n> > 10. The column vectors of $A$ form a basis for $\\mathbb{R}^n$.\n> > 11. The row vectors of $A$ are linearly independent.\n> > 12. The row vectors of $A$ span $\\mathbb{R}^n$.\n> > 13. The row vectors of $A$ form a basis for $\\mathbb{R}^n$\n> \n> > [!success] from Ch 4.3 Eigenvalues and Vectors\n> > 14. $\\det A \\neq 0$.\n> > 15. $0$ is not an eigenvalue of $A$."},
		{"id":"e27c6c4d43ea35cd","type":"text","text":"> [!tip] Theorem 3.29\n> \n> Let $S$ be a subspace of $\\mathbb{R}^n$ and let $\\mathcal{B} = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k\\}$ be a basis for $S$. \n> For every vector $\\mathbf{v}$ in $S$, there is exactly one way to write $\\mathbf{v}$ as a linear combination of the basis vectors in $\\mathcal{B}$:\n> $$\n> \\mathbf{v} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k\n> $$\n> \n\n### この定理の嬉しいポイント：\n\n- サブスペース内のすべてのベクトルが、基底の線形結合で書ける。\n- その線形結合の係数は他にない、唯一の方法で求められる。\n- これにより、サブスペースの構造を完全に把握できる。","x":6499,"y":8600,"width":647,"height":359},
		{"id":"119f2902bfe5db2c","type":"text","text":"> [!note] Definition\n> \n> Let $S$ be a subspace of $\\mathbb{R}^n$ and let $\\mathcal{B} = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k\\}$ be a basis for $S$. Let $\\mathbf{v}$ be a vector in $S$, and write \n> $$\n> \\mathbf{v} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k.\n> $$\n> Then $c_1, c_2, \\dots, c_k$ are called the **coordinates** of $\\mathbf{v}$ **with respect to** $\\mathcal{B}$, and the column vector\n> $$\n> [\\mathbf{v}]_{\\mathcal{B}} =\n> \\begin{bmatrix}\n> c_1 \\\\\n> c_2 \\\\\n> \\vdots \\\\\n> c_k\n> \\end{bmatrix}\n> $$\n> is the **coordinate vector** of $\\mathbf{v}$ with respect to $\\mathcal{B}$.\n> ","x":6555,"y":9041,"width":591,"height":410},
		{"id":"8a2771789f8495bb","x":12140,"y":4268,"width":983,"height":255,"type":"text","text":"> [!tip] Theorem 4.18\n> \n> Let $A$ be a square matrix with eigenvalue $\\lambda$ and corresponding eigenvector $\\mathbf{x}$.\n> \n> a. For any positive integer $n$, $\\lambda^n$ is an eigenvalue of $A^n$ with corresponding eigenvector $\\mathbf{x}$.\n> \n> b. If $A$ is invertible, then $\\frac{1}{\\lambda}$ is an eigenvalue of $A^{-1}$ with corresponding eigenvector $\\mathbf{x}$.\n> \n> c. If $A$ is invertible, then for any integer $n$, $\\lambda^n$ is an eigenvalue of $A^n$ with corresponding eigenvector $\\mathbf{x}$.\n> "},
		{"id":"360679315e94c0cc","x":12143,"y":4543,"width":858,"height":308,"type":"text","text":"> [!tip] Theorem 4.19\n> \n> Suppose the $n \\times n$ matrix $A$ has eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_m$ with corresponding eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_m$. If $\\mathbf{x}$ is a vector in $\\mathbb{R}^n$ that can be expressed as a linear combination of these eigenvectors—say, \n> \n> $$\n> \\mathbf{x} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_m \\mathbf{v}_m,\n> $$\n> \n> then, for any integer $k$, \n> \n> $$\n> A^k \\mathbf{x} = c_1 \\lambda_1^k \\mathbf{v}_1 + c_2 \\lambda_2^k \\mathbf{v}_2 + \\cdots + c_m \\lambda_m^k \\mathbf{v}_m.\n> $$\n> "},
		{"id":"3bd1c23ce172063a","x":9000,"y":5011,"width":560,"height":669,"type":"text","text":"## Similar Matrices\n> [!note] Definition: Similar Matrices\n> \n> Let $A$ and $B$ be $n \\times n$ matrices. We say that **$A$ is similar to $B$** if there is an invertible $n \\times n$ matrix $P$ such that \n> \n> $$\n> P^{-1}AP = B.\n> $$\n> \n> If $A$ is similar to $B$, we write \n> \n> $$\n> A \\sim B.\n> $$\n> \n\n> [!example] Example 4.22: Similar Matrices\n> \n> Let \n> $$\n> A = \n> \\begin{bmatrix}\n> 1 & 2 \\\\\n> 0 & -1\n> \\end{bmatrix}\n> \\quad \\text{and} \\quad \n> B = \n> \\begin{bmatrix}\n> 1 & 0 \\\\\n> -2 & -1\n> \\end{bmatrix}.\n> $$\n> \n> Then $A \\sim B$, since \n> \n> $$\n> \\begin{bmatrix}\n> 1 & 2 \\\\\n> 0 & -1\n> \\end{bmatrix}\n> \\begin{bmatrix}\n> 1 & -1 \\\\\n> 1 & 1\n> \\end{bmatrix}\n> = \n> \\begin{bmatrix}\n> 3 & 1 \\\\\n> -1 & -1\n> \\end{bmatrix}\n> = \n> \\begin{bmatrix}\n> 1 & -1 \\\\\n> 1 & 1\n> \\end{bmatrix}\n> \\begin{bmatrix}\n> 1 & 0 \\\\\n> -2 & -1\n> \\end{bmatrix}.\n> $$\n> \n> Thus, $AP = PB$ with \n> \n> $$\n> P = \n> \\begin{bmatrix}\n> 1 & -1 \\\\\n> 1 & 1\n> \\end{bmatrix}.\n> $$"},
		{"id":"f6d4de65e8b52007","x":9728,"y":5117,"width":492,"height":229,"type":"text","text":"> [!tip] Theorem 4.21: Properties of Similar Matrices\n> \n> Let $A$, $B$, and $C$ be $n \\times n$ matrices.\n> \n> a. $A \\sim A$\n> b. If $A \\sim B$, then $B \\sim A$.\n> c. If $A \\sim B$ and $B \\sim C$, then $A \\sim C$.\n\n"},
		{"id":"a9fea0518504618c","x":9883,"y":5953,"width":557,"height":527,"type":"text","text":"> [!tip] Theorem 4.22: Properties of Similar Matrices\n> \n> Let $A$ and $B$ be $n \\times n$ matrices with $A \\sim B$. Then:\n> \n> a. $\\det A = \\det B$\n> b. $A$ is invertible if and only if $B$ is invertible.\n> c. $A$ and $B$ have the same rank.\n> d. $A$ and $B$ have the same characteristic polynomial.\n> e. $A$ and $B$ have the same eigenvalues.\n> f. $A^m \\sim B^m$ for all integers $m \\geq 0$.\n> g. If $A$ is invertible, then $A^m \\sim B^m$ for all integers $m$.\n\nSimilarならば，，\n- detは同じ\n- A"}
	],
	"edges":[
		{"id":"27e3857078cd53b0","fromNode":"8aac3df908ad63c0","fromSide":"right","toNode":"3ff3bf252eab0d04","toSide":"left"},
		{"id":"eafa7d878728a55e","fromNode":"39ae9f27a0b3906d","fromSide":"right","toNode":"8aac3df908ad63c0","toSide":"left"},
		{"id":"3fe2c2cfc343c142","fromNode":"afe4f6e6d1a8c317","fromSide":"right","toNode":"f62f49400e1d88b7","toSide":"left"},
		{"id":"1c62e92101c42f06","fromNode":"2584069adee084f0","fromSide":"right","toNode":"2533422c74c97d56","toSide":"left"},
		{"id":"92d2e09c4ebc986e","fromNode":"2533422c74c97d56","fromSide":"right","toNode":"94a56f5d160f8d6a","toSide":"left"},
		{"id":"ea7086ebf827fdc4","fromNode":"406f7005658e7340","fromSide":"right","toNode":"1b4cd495a61a4ea7","toSide":"left"},
		{"id":"09ae6f58c555e9b9","fromNode":"a663a8355b44526b","fromSide":"right","toNode":"406f7005658e7340","toSide":"left"},
		{"id":"d695f6fb2d00e504","fromNode":"a663a8355b44526b","fromSide":"right","toNode":"20a028c8a7848aaf","toSide":"left"},
		{"id":"3744f8809f6fd32a","fromNode":"a663a8355b44526b","fromSide":"right","toNode":"3d5b418e6fec6764","toSide":"left"},
		{"id":"8194875cc02be835","fromNode":"3d5b418e6fec6764","fromSide":"right","toNode":"8f99bae5dbb985e8","toSide":"left"},
		{"id":"23eef5d7dcd827e1","fromNode":"0b42723ecde3a0fc","fromSide":"right","toNode":"f2faf3e9f5c1ff86","toSide":"left"},
		{"id":"243373b5848e7160","fromNode":"7821604b06949d71","fromSide":"right","toNode":"0f560b2e1a6094fb","toSide":"left"},
		{"id":"f45f9115c4526538","fromNode":"7821604b06949d71","fromSide":"right","toNode":"b7078196d6c5aa74","toSide":"left"},
		{"id":"295f2efe375f071f","fromNode":"63f325337c600605","fromSide":"bottom","toNode":"4c2e13bd5a11f092","toSide":"top"},
		{"id":"24b3b466edc1d597","fromNode":"63f325337c600605","fromSide":"bottom","toNode":"ca65d19aec8bef69","toSide":"top"},
		{"id":"dd5309f86b897141","fromNode":"afe4f6e6d1a8c317","fromSide":"top","toNode":"39ae9f27a0b3906d","toSide":"bottom"},
		{"id":"b6f33a5d109e32d7","fromNode":"2a12d16ef3f7ffb3","fromSide":"right","toNode":"4b85b11f1d1af140","toSide":"left"},
		{"id":"55d61a55125eaab3","fromNode":"2a12d16ef3f7ffb3","fromSide":"right","toNode":"7baa262b93ca3fb0","toSide":"left"},
		{"id":"209451defcd10e96","fromNode":"0d9d8a10a260ce79","fromSide":"right","toNode":"a0e17faa77656cf4","toSide":"left"},
		{"id":"04a62d37e589ed1d","fromNode":"d9278b565480d376","fromSide":"right","toNode":"e02bc77a41174546","toSide":"left"},
		{"id":"980d5e22f051575c","fromNode":"0d9d8a10a260ce79","fromSide":"right","toNode":"a77d8099dc1ff00d","toSide":"top"},
		{"id":"145a46a2d474744a","fromNode":"4090a8b66c9c27f9","fromSide":"bottom","toNode":"7821604b06949d71","toSide":"top"},
		{"id":"f825fc8b23f8094d","fromNode":"4090a8b66c9c27f9","fromSide":"bottom","toNode":"63f325337c600605","toSide":"top"},
		{"id":"05a71e54998fb983","fromNode":"b0fa401e849cc3e0","fromSide":"right","toNode":"5380508dd61fdeda","toSide":"left"},
		{"id":"f55fb51b635abc2a","fromNode":"07a436f13fd511ad","fromSide":"right","toNode":"84d9da5465d43d7f","toSide":"left"},
		{"id":"ff75dd7638265f46","fromNode":"07a436f13fd511ad","fromSide":"right","toNode":"7162ccdb2f765f6a","toSide":"left"},
		{"id":"bce94b10fefffe83","fromNode":"07a436f13fd511ad","fromSide":"right","toNode":"0625cfabd09b3b41","toSide":"left"},
		{"id":"054c30852a3a42cd","fromNode":"b0fa401e849cc3e0","fromSide":"bottom","toNode":"6c78954834f4b9ee","toSide":"left","toEnd":"none","color":"5","label":"似ている性質\nd ｜ c\n　e ｜ d, e"},
		{"id":"821049067239c210","fromNode":"92e56ea91c672fee","fromSide":"right","toNode":"491977904fc4fe14","toSide":"left"},
		{"id":"3847c2d4f1d57c90","fromNode":"92e56ea91c672fee","fromSide":"right","toNode":"b17bc95648034f23","toSide":"left"},
		{"id":"2d8e2197fcc85269","fromNode":"69cd7a2fdf06ac53","fromSide":"right","toNode":"78adc471de7b3e1d","toSide":"left"},
		{"id":"b934a12cd2457d5b","fromNode":"7162ccdb2f765f6a","fromSide":"bottom","toNode":"7dc2e6c4f186b230","toSide":"top","color":"3"},
		{"id":"3623d872af768913","fromNode":"78adc471de7b3e1d","fromSide":"right","toNode":"7dc2e6c4f186b230","toSide":"left"},
		{"id":"7128e3894e13b142","fromNode":"c6332f8a008274e1","fromSide":"right","toNode":"b4e3aa9afe4a736c","toSide":"top"},
		{"id":"aef8982c2b01109d","fromNode":"69cd7a2fdf06ac53","fromSide":"right","toNode":"d167bb3954894f7c","toSide":"left"},
		{"id":"3f0b0a3db15fd4d3","fromNode":"69cd7a2fdf06ac53","fromSide":"right","toNode":"c6332f8a008274e1","toSide":"left"},
		{"id":"65f4e31f38a0ada2","fromNode":"7162ccdb2f765f6a","fromSide":"right","toNode":"7e0b631421f2762f","toSide":"top"},
		{"id":"2958f0ef63033f38","fromNode":"da2fda8a2170e483","fromSide":"bottom","toNode":"a46da33bb3ea2c96","toSide":"top"},
		{"id":"6804ede9beee59f0","fromNode":"a46da33bb3ea2c96","fromSide":"bottom","toNode":"de29fa9c9e499c5b","toSide":"top"},
		{"id":"b94357586cd86cae","fromNode":"a46da33bb3ea2c96","fromSide":"bottom","toNode":"ab5b6a61adc38fb3","toSide":"top","toEnd":"none","color":"2"},
		{"id":"73ff871791b26ae6","fromNode":"da2fda8a2170e483","fromSide":"bottom","toNode":"921e676f8c0c19d5","toSide":"left","color":"1"},
		{"id":"2e7724c6aabad117","fromNode":"a2f0939b69b67762","fromSide":"right","toNode":"d1a14f4994db99e3","toSide":"left","toEnd":"none"},
		{"id":"158ae4021eeae621","fromNode":"b873c13ca87064e6","fromSide":"right","toNode":"bd265e541203289d","toSide":"left","toEnd":"none"},
		{"id":"9846dfcbc3dd04a3","fromNode":"b873c13ca87064e6","fromSide":"right","toNode":"8289f5873de50f99","toSide":"left","toEnd":"none"},
		{"id":"67f0f7e92f2df23c","fromNode":"921e676f8c0c19d5","fromSide":"bottom","toNode":"04bb1535b901137d","toSide":"left"},
		{"id":"6a2b97aa6acaf187","fromNode":"d0a5da3d7d0b449c","fromSide":"right","toNode":"062dd0c345264ccf","toSide":"left","toEnd":"none"},
		{"id":"d53947032ce269eb","fromNode":"b873c13ca87064e6","fromSide":"right","toNode":"32b8ca1c9dae8ad6","toSide":"left","toEnd":"none"},
		{"id":"208bec3c9f17f360","fromNode":"da2fda8a2170e483","fromSide":"bottom","toNode":"88584b7db335d6ac","toSide":"left","color":"1"},
		{"id":"a476e4acfe0b8d33","fromNode":"88584b7db335d6ac","fromSide":"bottom","toNode":"aaeeed1e2b5b4a8b","toSide":"top","toEnd":"none","color":"6"},
		{"id":"bc8b4278c433159b","fromNode":"921e676f8c0c19d5","fromSide":"right","toNode":"ed540d9732ecb6be","toSide":"top","toEnd":"none","color":"3"},
		{"id":"cfc43291d79882a6","fromNode":"7a7ab5e8b1fbe002","fromSide":"right","toNode":"921e676f8c0c19d5","toSide":"top","toEnd":"none","color":"5"},
		{"id":"678f9302ffb379a7","fromNode":"878cd89d922f980f","fromSide":"right","toNode":"921e676f8c0c19d5","toSide":"top","toEnd":"none","color":"3"},
		{"id":"78c5210f5a711d08","fromNode":"4ded22cbe82a3ab3","fromSide":"right","toNode":"88584b7db335d6ac","toSide":"top","toEnd":"none","color":"6"},
		{"id":"5d1c62016d825226","fromNode":"921e676f8c0c19d5","fromSide":"right","toNode":"feee36a19b552471","toSide":"top","toEnd":"none","color":"5"},
		{"id":"e680e819967aa3ed","fromNode":"aaeeed1e2b5b4a8b","fromSide":"bottom","toNode":"b1ca56510c7dcb45","toSide":"top","toEnd":"none","color":"6"},
		{"id":"c51dcd4c1a6de018","fromNode":"0d9d8a10a260ce79","fromSide":"right","toNode":"cea6515eb2eaa635","toSide":"left"},
		{"id":"8bb654dfa3f9418d","fromNode":"408ea514ecdd2a0b","fromSide":"right","toNode":"92e56ea91c672fee","toSide":"left","label":"standard basis\ne1, e2, e3, .... , en in R^n"},
		{"id":"c3e5adc946b816f1","fromNode":"04bb1535b901137d","fromSide":"right","toNode":"cb9d20df62840edf","toSide":"bottom","toEnd":"none"},
		{"id":"4cc5c3876c372a41","fromNode":"da2fda8a2170e483","fromSide":"right","toNode":"99587d1145bd6959","toSide":"left","toEnd":"none","color":"#37444c"},
		{"id":"795cf3891375e55e","fromNode":"63fa56a516280068","fromSide":"right","toNode":"8b124a8252aeb0bc","toSide":"left","color":"4"},
		{"id":"01dec0d9fafd1712","fromNode":"e27c6c4d43ea35cd","fromSide":"bottom","toNode":"119f2902bfe5db2c","toSide":"top"},
		{"id":"a13811d945b022a9","fromNode":"5342d19226984252","fromSide":"right","toNode":"b4568121d8c03310","toSide":"bottom"},
		{"id":"38b2795ae633cd37","fromNode":"ac93ec546cb9e8af","fromSide":"right","toNode":"0d68b779dd60a842","toSide":"left"},
		{"id":"b39d4b9753556d3a","fromNode":"0d68b779dd60a842","fromSide":"right","toNode":"28035385817fbc35","toSide":"left"},
		{"id":"0c1718d436ae92c8","fromNode":"28035385817fbc35","fromSide":"bottom","toNode":"87c0aeea841a31ec","toSide":"top"},
		{"id":"e684aab53d7eac3f","fromNode":"408e5b9d635922c5","fromSide":"top","toNode":"0d68b779dd60a842","toSide":"bottom","toEnd":"none","color":"3"},
		{"id":"b820e7a2de0f4df6","fromNode":"7e9bf228246f0519","fromSide":"right","toNode":"87c0aeea841a31ec","toSide":"bottom","toEnd":"none","color":"3"},
		{"id":"81de5f74c1a1d5a8","fromNode":"7e9bf228246f0519","fromSide":"right","toNode":"f762fb88b5ab3176","toSide":"bottom","toEnd":"none","color":"3"},
		{"id":"a6d5c945c4a01eee","fromNode":"7e9bf228246f0519","fromSide":"right","toNode":"e3b3efdc79268161","toSide":"left","toEnd":"none","color":"3"},
		{"id":"9f7490a7680bb09b","fromNode":"78adc471de7b3e1d","fromSide":"bottom","toNode":"78d29895fb0aba15","toSide":"top","color":"#0033ff"},
		{"id":"9b8959e6eb2e28d3","fromNode":"78d29895fb0aba15","fromSide":"right","toNode":"3e0bfe748403fe5e","toSide":"bottom","color":"#0062ff"},
		{"id":"a93e1d6378b278d2","fromNode":"3bd1c23ce172063a","fromSide":"right","toNode":"f6d4de65e8b52007","toSide":"left","toEnd":"none"}
	]
}