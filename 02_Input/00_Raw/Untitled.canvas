{
	"nodes":[
		{"id":"43c94965f5225f6b","type":"group","x":86,"y":100,"width":4894,"height":2862,"color":"5","label":"# Vectors"},
		{"id":"fb38bf205352f2e2","type":"group","x":2140,"y":285,"width":2840,"height":1685,"color":"3","label":"## 1.2 Length and Angle: The Dot Product## Á∑ö„Å®ËßíÂ∫¶"},
		{"id":"8137fc4b9b6eabad","type":"group","x":-540,"y":3420,"width":2452,"height":1875,"color":"3","label":"3.1 Matrix Operations"},
		{"id":"22c968707bffca50","type":"group","x":86,"y":1720,"width":1953,"height":1242,"color":"3","label":"## 1.3 Lines and Planes"},
		{"id":"c675cb1bcbe584db","type":"group","x":200,"y":285,"width":1725,"height":1215,"color":"3","label":"## 1.1 The Geometry and Algebra of Vectors"},
		{"id":"34dd65c935509024","type":"group","x":-540,"y":5520,"width":1489,"height":1183,"color":"3","label":"3.2 Matrix Algebra"},
		{"id":"406f7005658e7340","type":"text","text":"> [!important] Theorem 1.3\n> Let $\\mathbf{v}$ be a vector in $\\mathbb{R}^n$ and let $c$ be a scalar. Then:\n> \n> 1.  \n>    $$\n>    \\|\\mathbf{v}\\| = 0 \\iff \\mathbf{v} = \\mathbf{0}\n>    $$\n> \n> 2.  \n>    $$\n>    \\|c\\mathbf{v}\\| = |c| \\|\\mathbf{v}\\|\n>    $$\n> ","x":2869,"y":805,"width":469,"height":280},
		{"id":"20a028c8a7848aaf","type":"text","text":"> [!important] Theorem 1.4\n> #### The Cauch-schwarz Inequality\n> For all vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, \n>\n> $$\n> |\\mathbf{u} \\cdot \\mathbf{v}| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|.\n> $$\n\n","x":2869,"y":1115,"width":469,"height":220},
		{"id":"a663a8355b44526b","type":"text","text":"### Length\n\n> [!error] Length\n> The **length (or norm)** of a vector \n> \n> $$\n> \\mathbf{v} =\n> \\begin{bmatrix}\n> v_1 \\\\\n> v_2 \\\\\n> \\vdots \\\\\n> v_n\n> \\end{bmatrix}\n> $$\n> \n> in $\\mathbb{R}^n$ is the nonnegative scalar $\\|\\mathbf{v}\\|$ defined by:\n> \n> $$\n> \\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}.\n> $$\n> ","x":2191,"y":765,"width":517,"height":380},
		{"id":"3d5b418e6fec6764","type":"text","text":"> [!important] Theorem 1.5\n> #### The Triangle Inequlity\n> For all vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, \n> \n> $$\n> \\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|.\n> $$","x":2893,"y":1365,"width":421,"height":213},
		{"id":"3219b07f43b22a12","type":"text","text":"### Angles\n> [!error] Angles\n> For nonzero vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, \n> \n> $$\n> \\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}.\n> $$\n\n","x":2184,"y":1445,"width":400,"height":240},
		{"id":"f2faf3e9f5c1ff86","type":"text","text":"> [!important] Theorem 1.6\n> #### Pythagoras' Theorem  („Éî„Çø„Ç¥„É©„Çπ„ÅÆÂÆöÁêÜ)\n> For all vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, \n> \n> $$\n> \\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2\n> $$\n> \n> if and only if $\\mathbf{u}$ and $\\mathbf{v}$ are orthogonal.\n","x":2882,"y":1695,"width":480,"height":255},
		{"id":"0b42723ecde3a0fc","type":"text","text":"### Orthogonal Vectors\n\n> [!error] Orthogonal Vectors\n> Two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$ are **orthogonal** to each other if:\n> $$\n> \\mathbf{u} \\cdot \\mathbf{v} = 0.\n> $$\n","x":2162,"y":1712,"width":609,"height":220},
		{"id":"58911c58cb510c61","type":"text","text":"> [!todo] ÁêÜËß£„Åó„Å¶„Å™„ÅÑ üí©","x":3213,"y":1305,"width":250,"height":60},
		{"id":"1b4cd495a61a4ea7","type":"text","text":"##### Proof\nÂëΩÈ°å a:\n$$\n\\|\\mathbf{v}\\| = 0 \\iff \\mathbf{v} = \\mathbf{0}\n$$\n\n- **ÂøÖË¶ÅÊù°‰ª∂ $(\\Rightarrow)$**:  \n\n  „Éé„É´„É†„ÅÆÂÆöÁæ©„Çí‰Ωø„ÅÑ„ÄÅ$\\|\\mathbf{v}\\| = 0$ „ÅÆ„Å®„Åç $\\mathbf{v} \\cdot \\mathbf{v} = 0$ „Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ§∫„Åô„ÄÇ  \n  ÂÜÖÁ©ç„Åå„Çº„É≠„Å´„Å™„Çã„ÅÆ„ÅØ $\\mathbf{v}$ „Åå„Çº„É≠„Éô„ÇØ„Éà„É´„ÅÆÂ†¥Âêà„ÅÆ„Åø„ÄÇ\n\n- **ÂçÅÂàÜÊù°‰ª∂ $(\\Leftarrow)$**:  \n  $\\mathbf{v} = \\mathbf{0}$ „Åß„ÅÇ„Çå„Å∞„ÄÅ„Éé„É´„É†„ÅÆÂÆöÁæ©„Çà„Çä $\\|\\mathbf{v}\\| = 0$ „ÅåÊàêÁ´ã„Åô„Çã„ÄÇ\n\n---\n\nÂëΩÈ°å b:\n$$\n\\|c\\mathbf{v}\\| = |c| \\|\\mathbf{v}\\|\n$$\n\n- „Éé„É´„É†„ÅÆÂÆöÁæ©„Çí‰Ωø„ÅÜÔºö\n$$\n\\|c\\mathbf{v}\\| = \\sqrt{(c\\mathbf{v}) \\cdot (c\\mathbf{v})}.\n$$\n- ÂÜÖÁ©ç„ÅÆ„Çπ„Ç´„É©„ÉºÂÄç„ÅÆÊÄßË≥™„ÇíÈÅ©Áî®„Åó„ÄÅ\n$$\n(c\\mathbf{v}) \\cdot (c\\mathbf{v}) = c^2 (\\mathbf{v} \\cdot \\mathbf{v})\n$$\n„ÇíÁ¢∫Ë™ç„Åô„Çã„ÄÇ\n- $\\sqrt{c^2} = |c|$ „Çí‰Ωø„ÅÑ„ÄÅÊúÄÁµÇÁöÑ„Å´„Éé„É´„É†„ÅÆÂºè„ÇíÂæó„Çã„ÄÇ\n","x":3513,"y":685,"width":545,"height":660},
		{"id":"2533422c74c97d56","type":"text","text":"> [!important] Theorem 1.2\n> Let $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{w}$ be vectors in $\\mathbb{R}^n$, and let $c$ be a scalar. Then:\n> \n> 1. **Commutativity**  \n>    $$\n>    \\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u}\n>    $$\n> \n> 2. **Distributivity**  \n>    $$\n>    \\mathbf{u} \\cdot (\\mathbf{v} + \\mathbf{w}) = \\mathbf{u} \\cdot \\mathbf{v} + \\mathbf{u} \\cdot \\mathbf{w}\n>    $$\n> \n> 3. **Scalar Multiplication**  \n>    $$\n>    (c\\mathbf{u}) \\cdot \\mathbf{v} = c(\\mathbf{u} \\cdot \\mathbf{v})\n>    $$\n> \n> 4. **Non-Negativity and Zero Condition**  \n>    $$\n>    \\mathbf{u} \\cdot \\mathbf{u} \\geq 0, \\quad \\mathbf{u} \\cdot \\mathbf{u} = 0 \\text{ if and only if } \\mathbf{u} = \\mathbf{0}.\n>    $$\n","x":2813,"y":305,"width":549,"height":460},
		{"id":"78bab4ed6f7caf9d","type":"text","text":"### Distance\n> [!error] Distance\n> The **distance** $d(\\mathbf{u}, \\mathbf{v})$ between vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$ is defined by:\n> \n> $$\n> d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|.\n> $$\n","x":2160,"y":1175,"width":580,"height":240},
		{"id":"8f99bae5dbb985e8","type":"text","text":"##### Proof\n$$ \\|\\mathbf{u} + \\mathbf{v}\\| ^2 $$ „Åã„ÇâÂßã„Çã„Å®Ë®ºÊòé„Åß„Åç„Çã\n","x":3513,"y":1445,"width":289,"height":145},
		{"id":"94a56f5d160f8d6a","type":"text","text":"##### Proof\nÊñπÈáùÔºö„Åù„Çå„Åû„Çå„ÅÆ„Éô„ÇØ„Éà„É´„Çí„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÅßË°®Áèæ„Åó„Å¶ÔºåÂ∑¶Ëæ∫„Åã„ÇâÂè≥Ëæ∫Ôºå„ÇÇ„Åó„Åè„ÅØ„Åù„ÅÆÂèçÂØæ„ÅåÊàê„ÇäÁ´ã„Å§„Åã„ÇíË®ºÊòé„Åô„ÇãÔºé","x":3536,"y":420,"width":250,"height":230},
		{"id":"2584069adee084f0","type":"text","text":"### The Dot Product\n> [!error] Dot Product\n> If\n> $$\n> \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix}, \\quad\n> \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix},\n> $$\n> then the **dot product** is defined as\n> $$\n> \\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n.\n> $$\n> ","x":2191,"y":355,"width":437,"height":370},
		{"id":"696e413b6a7aee6c","type":"text","text":"### Projection\n\nConsider two nonzero vectors $\\mathbf{u}$ and $\\mathbf{v}$. Let $\\mathbf{p}$ be the vector obtained by dropping a perpendicular from the head of $\\mathbf{v}$ onto $\\mathbf{u}$, and let $\\theta$ be the angle between $\\mathbf{u}$ and $\\mathbf{v}$.\n\n![[projection.png]]\n\nThen clearly:\n\n$$\n\\mathbf{p} = \\|\\mathbf{p}\\| \\hat{\\mathbf{u}},\n$$\n\nwhere $\\hat{\\mathbf{u}} = \\frac{1}{\\|\\mathbf{u}\\|}\\mathbf{u}$ is the unit vector in the direction of $\\mathbf{u}$. Moreover, elementary trigonometry gives:\n\n$$\n\\|\\mathbf{p}\\| = \\|\\mathbf{v}\\| \\cos \\theta,\n$$\n\nand we know that:\n\n$$\n\\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}.\n$$\n\nThus, after substitution, we obtain:\n\n$$\n\\mathbf{p} = \\|\\mathbf{v}\\| \n\\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|} \\right)\n\\left( \\frac{1}{\\|\\mathbf{u}\\|} \\right) \\mathbf{u}.\n$$\n\n$$\n= \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|^2} \\right) \\mathbf{u}.\n$$\n\n$$\n= \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{u} \\cdot \\mathbf{u}} \\right) \\mathbf{u}.\n$$\n\n> [!error] Definition\n> If $\\mathbf{u}$ and $\\mathbf{v}$ are vectors in $\\mathbb{R}^n$ and $\\mathbf{u} \\neq \\mathbf{0}$, then the **projection of $\\mathbf{v}$ onto $\\mathbf{u}$** is the vector $\\text{proj}_{\\mathbf{u}}(\\mathbf{v})$ defined by:\n> \n> $$\n> \\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{u} \\cdot \\mathbf{u}} \\right) \\mathbf{u}.\n> $$","x":4160,"y":330,"width":800,"height":880},
		{"id":"b2a67b5f4d60d2d6","type":"text","text":"## 1.2 Length and Angle: The Dot Product\n## Á∑ö„Å®ËßíÂ∫¶","x":2134,"y":120,"width":500,"height":100},
		{"id":"3ff3bf252eab0d04","type":"text","text":"##### Proof\nÊñπÈáùÔºö„Åù„Çå„Åû„Çå„ÅÆ„Éô„ÇØ„Éà„É´„Çí„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÅßË°®Áèæ„Åó„Å¶ÔºåÂ∑¶Ëæ∫„Åã„ÇâÂè≥Ëæ∫Ôºå„ÇÇ„Åó„Åè„ÅØ„Åù„ÅÆÂèçÂØæ„ÅåÊàê„ÇäÁ´ã„Å§„Åã„ÇíË®ºÊòé„Åô„ÇãÔºé","x":1655,"y":598,"width":250,"height":230},
		{"id":"8aac3df908ad63c0","type":"text","text":"> [!important] Theorem 1.1 Algebraic Properties of Vector in $\\mathbb{R}^n$\n> Let $\\vec{u}$, $\\vec{v}$, and $\\vec{w}$ be vectors in $\\mathbb{R}^n$\n> Let $c$ and $d$ be scalars.<br>\n> Then:\n> 1. **Commutativity:**\n>    $$\n>    \\vec{u} + \\vec{v} = \\vec{v} + \\vec{u}\n>    $$\n> \n> 2. **Associativity:**\n>    $$\n>    (\\vec{u} + \\vec{v}) + \\vec{w} = \\vec{u} + (\\vec{v} + \\vec{w})\n>    $$\n> \n> 3. **Identity Element:**\n>    $$\n>    \\vec{u} + \\vec{0} = \\vec{u}\n>    $$\n> \n> 4. **Inverse Element:**\n>    $$\n>    \\vec{u} + (-\\vec{u}) = \\vec{0}\n>    $$\n> \n> 5. **Distributivity of Scalars over Vector Addition:**\n>    $$\n>    c(\\vec{u} + \\vec{v}) = c\\vec{u} + c\\vec{v}\n>    $$\n> \n> 6. **Distributivity of Scalars over Scalar Addition:**\n>    $$\n>    (c + d)\\vec{u} = c\\vec{u} + d\\vec{u}\n>    $$\n> \n> 7. **Associativity of Scalar Multiplication:**\n>    $$\n>    c(d\\vec{u}) = (cd)\\vec{u}\n>    $$\n> \n> 8. **Identity Scalar:**\n>    $$\n>    1\\vec{u} = \\vec{u}\n>    $$","x":862,"y":305,"width":619,"height":815},
		{"id":"f62f49400e1d88b7","type":"text","text":"### Linear Combinations\n> [!danger] Linear Combinations\n> Let $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k \\in \\mathbb{R}^n$. <br>\n> A vector $\\mathbf{v} \\in \\mathbb{R}^n$ is called  $\\textbf{linear combination}$ <br>\n> if there exists a set of scalars $c_1, c_2, \\ldots, c_k \\in \\mathbb{R}$ such that:\n> $$\n> \\mathbf{v} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_k \\mathbf{v}_k.\n> $$","x":877,"y":1190,"width":500,"height":290},
		{"id":"39ae9f27a0b3906d","type":"text","text":"### Vectors in $\\mathbb{R}^n$","x":480,"y":890,"width":200,"height":70},
		{"id":"afe4f6e6d1a8c317","type":"text","text":"## 1.1 The Geometry and Algebra of Vectors","x":220,"y":1190,"width":520,"height":70},
		{"id":"4090a8b66c9c27f9","type":"text","text":"## 1.3 Lines and Planes","x":400,"y":1965,"width":318,"height":70},
		{"id":"0f560b2e1a6094fb","type":"text","text":"### Normal Form of a Line  \n> [!caution] $\\mathbb{R}^2$\n> \n> > [!error] Normal Form\n> > The **normal form of the equation of a line** $\\ell$ in $\\mathbb{R}^2$ is:\n> > \n> > $$\n> > \\mathbf{n} \\cdot (\\mathbf{x} - \\mathbf{p}) = 0\n> > $$\n> > \n> > or equivalently:\n> > \n> > $$\n> > \\mathbf{n} \\cdot \\mathbf{x} = \\mathbf{n} \\cdot \\mathbf{p},\n> > $$\n> > \n> > where $\\mathbf{p}$ is a specific point on $\\ell$ and $\\mathbf{n} \\neq \\mathbf{0}$ is a normal vector for $\\ell$.\n> \n> > [!tip] General Form\n> > The **general form of the equation of $\\ell$** is:\n> > \n> > $$\n> > ax + by = c,\n> > $$\n> > \n> > where $\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\end{bmatrix}$ is a normal vector for $\\ell$.\n> ","x":1382,"y":1740,"width":637,"height":623},
		{"id":"b7078196d6c5aa74","type":"text","text":"### Vector Form of a Line\n> [!caution] $\\mathbb{R}^2$ and $\\mathbb{R}^3$\n> \n> > [!error] Vector Form\n> > The **vector form of the equation of a line** $\\ell$ in $\\mathbb{R}^2$ or $\\mathbb{R}^3$ is:\n> > \n> > $$\n> > \\mathbf{x} = \\mathbf{p} + t \\mathbf{d},\n> > $$\n> > \n> > where $\\mathbf{p}$ is a specific point on $\\ell$ and $\\mathbf{d} \\neq \\mathbf{0}$ is a direction vector for $\\ell$.\n>  \n> > [!tip] Parametric Equations\n> > The equations corresponding to the components of the vector form of the equation are called **parametric equations** of $\\ell$.\n","x":1382,"y":2384,"width":621,"height":504},
		{"id":"8e941bba141b5a51","type":"text","text":"# Matrix","x":-1520,"y":4000,"width":250,"height":60},
		{"id":"7821604b06949d71","type":"text","text":"## Lines in $\\mathbb{R}^2$, $\\mathbb{R}^3$","x":821,"y":2165,"width":212,"height":62},
		{"id":"63f325337c600605","type":"text","text":"## Planes in $\\mathbb{R}^3$","x":390,"y":2202,"width":196,"height":50},
		{"id":"4c2e13bd5a11f092","type":"text","text":"### Normal Form of a Plane\n> [!info] Normal Form\n> The **normal form of the equation of a plane** $\\mathcal{P}$ in $\\mathbb{R}^3$ is:\n> \n> $$\n> \\mathbf{n} \\cdot (\\mathbf{x} - \\mathbf{p}) = 0\n> $$\n> \n> or equivalently:\n> \n> $$\n> \\mathbf{n} \\cdot \\mathbf{x} = \\mathbf{n} \\cdot \\mathbf{p},\n> $$\n> \n> where $\\mathbf{p}$ is a specific point on $\\mathcal{P}$ and $\\mathbf{n} \\neq \\mathbf{0}$ is a normal vector for $\\mathcal{P}$.\n\n> [!tip] General Form\n> The **general form of the equation of $\\mathcal{P}$** is:\n> \n> $$\n> ax + by + cz = d,\n> $$\n> \n> where $\\mathbf{n} = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}$ is a normal vector for $\\mathcal{P}$.\n","x":106,"y":2384,"width":569,"height":558},
		{"id":"ca65d19aec8bef69","type":"text","text":"### Vector Form of a Plane\n> [!info] Vector Form\n> The **vector form of the equation of a plane** $\\mathcal{P}$ in $\\mathbb{R}^3$ is:\n> \n> $$\n> \\mathbf{x} = \\mathbf{p} + s \\mathbf{u} + t \\mathbf{v},\n> $$\n> \n> where $\\mathbf{p}$ is a point on $\\mathcal{P}$ and $\\mathbf{u}$ and $\\mathbf{v}$ are direction vectors for $\\mathcal{P}$ ( $\\mathbf{u}$ and $\\mathbf{v}$ are nonzero and parallel to $\\mathcal{P}$, but not parallel to each other).\n\n> [!tip] Parametric Equations\n> The equations corresponding to the components of the vector form of the equation are called **parametric equations** of $\\mathcal{P}$.\n","x":710,"y":2444,"width":569,"height":438},
		{"id":"db2670541536258e","type":"text","text":"## 3.1 Matrix Operations\n### Ë®àÁÆóÊñπÊ≥ï\n","x":-520,"y":3440,"width":303,"height":120},
		{"id":"e02bc77a41174546","type":"text","text":"> [!example] Example 3.2\n> Consider the matrices:\n> \n> $$\n> R = [1 \\ 4 \\ 3]\n> $$\n> \n> and\n> \n> $$\n> C = \\begin{bmatrix} 1 \\\\ 4 \\\\ 3 \\end{bmatrix}.\n> $$\n> \n> Despite the fact that $R$ and $C$ have the same entries in the same order, $R \\neq C$ since $R$ is $1 \\times 3$ and $C$ is $3 \\times 1$. (If we read $R$ and $C$ aloud, they both sound the same: ‚Äúone, four, three.‚Äù) Thus, our distinction between row matrices/vectors and column matrices/vectors is an important one.\n","x":56,"y":3440,"width":528,"height":445},
		{"id":"d9278b565480d376","type":"text","text":"> [!note] Definition\n> A **matrix** is a rectangular array of numbers called the **entries**, or **elements**, of the matrix.\n","x":-444,"y":3662,"width":393,"height":166},
		{"id":"6c9983081b02a650","type":"text","text":"### Matrix Multiplication\n\n> [!note] Definition\n> If $A$ is an $m \\times n$ matrix and $B$ is an $n \\times r$ matrix, then the **product** $C = AB$ is an $m \\times r$ matrix. \n> The $(i, j)$ entry of the product is computed as follows:\n> \n> $$\n> c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \\cdots + a_{in}b_{nj}.\n> $$\n\n> [!example] Example 3.6\n> Compute $AB$ if:\n> \n> $$\n> A = \\begin{bmatrix} 1 & 3 & -1 \\\\ -2 & -1 & 1 \\end{bmatrix}, \\quad\n> B = \\begin{bmatrix} -4 & 0 & 3 & -1 \\\\ 5 & -2 & -1 & 1 \\\\ -1 & 2 & 0 & 6 \\end{bmatrix}.\n> $$\n> \n> Then:\n> \n> $$\n> AB = \\begin{bmatrix} 12 & -8 & 0 & -4 \\\\ 2 & 4 & -5 & 7 \\end{bmatrix}.\n> $$\n","x":-444,"y":3980,"width":563,"height":551},
		{"id":"2a12d16ef3f7ffb3","type":"text","text":"> [!example]\n> $$\n> A = \\begin{bmatrix} 1 & 4 & 0 \\\\ -2 & 6 & 5 \\end{bmatrix}, \\quad\n> B = \\begin{bmatrix} -3 & 1 & -1 \\\\ 3 & 0 & 2 \\end{bmatrix}, \\quad \\text{and} \\quad\n> C = \\begin{bmatrix} 4 & 3 \\\\ 2 & 1 \\end{bmatrix}.\n> $$\n","x":636,"y":3620,"width":508,"height":140},
		{"id":"a0e17faa77656cf4","type":"text","text":"#### a. Induction„ÅßËß£„Åè„Éë„Çø„Éº„É≥ÔºàÁéãÈÅìÔºâÊôÇÈñì„Åã„Åã„Çã\n> [!example]\n> Let A be a matrix\n> \n> $$\n> A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix},\n> $$\n> \n> then:\n> \n> $$\n> A^2 = A \\cdot A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix},\n> $$\n> \n> $$\n> A^3 = A^2 \\cdot A = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 4 & 4 \\\\ 4 & 4 \\end{bmatrix}.\n> $$\n> \n> In general:\n> \n> $$\n> A^n = \\begin{bmatrix} 2^{n-1} & 2^{n-1} \\\\ 2^{n-1} & 2^{n-1} \\end{bmatrix} \\quad \\text{for all } n \\geq 1.\n> $$\n> \n> ---\n> \n> ##### Proof by Mathematical Induction:\n> \n> 1. **Base Case (\\(n = 1\\))**:  \n>    When \\(n = 1\\),\n> \n>    $$\n>    A^1 = \\begin{bmatrix} 2^{1-1} & 2^{1-1} \\\\ 2^{1-1} & 2^{1-1} \\end{bmatrix} = \\begin{bmatrix} 2^0 & 2^0 \\\\ 2^0 & 2^0 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = A.\n>    $$\n> \n>    Thus, the base case holds.\n> \n> 2. **Induction Hypothesis:**  \n>    Assume that the formula holds for \\(n = k\\), i.e.,\n> \n>    $$\n>    A^k = \\begin{bmatrix} 2^{k-1} & 2^{k-1} \\\\ 2^{k-1} & 2^{k-1} \\end{bmatrix}.\n>    $$\n> \n> 3. **Induction Step:**  \n>    To prove the formula holds for \\(n = k+1\\), compute \\(A^{k+1} = A^k \\cdot A\\):\n> \n>    $$\n>    A^{k+1} = \\begin{bmatrix} 2^{k-1} & 2^{k-1} \\\\ 2^{k-1} & 2^{k-1} \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}.\n>    $$\n> \n>    Performing the matrix multiplication:\n> \n>    $$\n>    A^{k+1} = \\begin{bmatrix} 2^{k} & 2^{k} \\\\ 2^{k} & 2^{k} \\end{bmatrix}.\n>    $$\n> \n>    Simplifying further:\n> \n>    $$\n>    A^{k+1} = \\begin{bmatrix} 2^{(k+1)-1} & 2^{(k+1)-1} \\\\ 2^{(k+1)-1} & 2^{(k+1)-1} \\end{bmatrix}.\n>    $$\n> \n> Thus, the formula holds for all \\(n \\geq 1\\) by the principle of mathematical induction.\n","x":956,"y":4035,"width":700,"height":1240},
		{"id":"4b85b11f1d1af140","type":"text","text":"### Addition\n\n> [!example] Example 3.3\n> $$\n> A + B = \\begin{bmatrix} -2 & 5 & -1 \\\\ 1 & 6 & 7 \\end{bmatrix}.\n> $$\n> \n> but neither $A + C$ nor $B + C$ is defined.\n","x":1252,"y":3460,"width":440,"height":240},
		{"id":"7baa262b93ca3fb0","type":"text","text":"### Scalar Multiplication\n> [!example] Example 3.4\n> For matrix $A$ in [Example 3.3](#example-3.3),\n> \n> $$\n> 2A = \\begin{bmatrix} 2 & 8 & 0 \\\\ -4 & 12 & 10 \\end{bmatrix}, \\quad\n> \\frac{1}{2}A = \\begin{bmatrix} \\frac{1}{2} & 2 & 0 \\\\ -1 & 3 & \\frac{5}{2} \\end{bmatrix}, \\quad\n> \\text{and} \\quad (-1)A = \\begin{bmatrix} -1 & -4 & 0 \\\\ 2 & -6 & -5 \\end{bmatrix}.\n> $$\n","x":1252,"y":3740,"width":640,"height":240},
		{"id":"0d9d8a10a260ce79","type":"text","text":"### Matrix Powers","x":312,"y":4027,"width":207,"height":84},
		{"id":"a77d8099dc1ff00d","type":"text","text":"#### b. Âæ™Áí∞„ÅåË¶ã„Å§„Åã„Çã„Éë„Çø„Éº„É≥\n\n> [!example]\n> If \n> $$\n> B = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix},\n> $$\n> \n> then:\n> \n> $$\n> B^2 = B \\cdot B = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n> $$\n> \n> Continuing, we find:\n> \n> $$\n> B^3 = B^2 \\cdot B = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix},\n> $$\n> \n> and:\n> \n> $$\n> B^4 = B^3 \\cdot B = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n> $$\n> \n> Thus:\n> \n> $$\n> B^5 = B, \n> $$\n> \n> and the sequence of powers of \\( B \\) repeats in a cycle of four:\n> \n> $$\n> \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}, \\quad\n> \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}, \\quad\n> \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}, \\quad\n> \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\dots\n> $$\n","x":312,"y":4588,"width":539,"height":687},
		{"id":"1fe99bf9f314d746","type":"text","text":"### Transpose\n> [!example] Example 3.14\n> Let\n> \n> $$\n> A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 5 & 0 & 1 \\end{bmatrix}, \\quad\n> B = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\quad\n> \\text{and} \\quad C = \\begin{bmatrix} 5 & -1 & 2 \\end{bmatrix}.\n> $$\n> \n> Then their transposes are:\n> \n> $$\n> A^T = \\begin{bmatrix} 1 & 5 \\\\ 3 & 0 \\\\ 2 & 1 \\end{bmatrix}, \\quad\n> B^T = \\begin{bmatrix} a & c \\\\ b & d \\end{bmatrix}, \\quad\n> \\text{and} \\quad C^T = \\begin{bmatrix} 5 \\\\ -1 \\\\ 2 \\end{bmatrix}.\n> $$\n","x":-444,"y":4600,"width":646,"height":380},
		{"id":"8219f831696f38e6","type":"text","text":"### Symmetric\n> [!note] Definition\n> A square matrix $A$ is **symmetric** if $A^T = A$ ‚Äî that is, if $A$ is equal to its own transpose.\n","x":-444,"y":5020,"width":445,"height":200},
		{"id":"d9cccd556dd0287f","type":"text","text":"## 3.2 Matrix Algebra\n### ‰ª£Êï∞ÁöÑ„Å™ÊÄßË≥™\n","x":-520,"y":5540,"width":272,"height":91},
		{"id":"d885407eac2e438f","type":"text","text":"> [!tip]\n> ### Property of Matrix Addition\n> > [!warning] Âä†ÁÆó„ÅØÂêå„Åò„Çµ„Ç§„Ç∫„Åß„ÅÇ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã\n> \n> Let $A$, $B$, and $C$ be matrices of the same size:\n> \n> a. $A + B = B + A$ (Commutativity)\n> \n> b. $(A + B) + C = A + (B + C)$ (Associativity)\n> \n> c. $A + O = A$\n> \n> d. $A + (-A) = O$\n","x":-506,"y":5663,"width":440,"height":420},
		{"id":"939ae90b31e90842","type":"text","text":"> [!tip]\n> ### Property of Scalar Multiplication\n> Let $A$ be a matrix and $c$, $d$ be scalars:\n> \n> e. $c(A + B) = cA + cB$ (Distributivity)\n> \n> f. $(c + d)A = cA + dA$ (Distributivity)\n> \n> g. $c(dA) = (cd)A$\n> \n> h. $1A = A$","x":-52,"y":5667,"width":411,"height":376},
		{"id":"b0fa401e849cc3e0","type":"text","text":"> [!tip]\n> ### Property of Transpose\n> Let $A$ and $B$ be matrices (whose sizes are such that the indicated operations can be performed) \n> Let $k$ be a scalar. \n> Then:<br>\n> \n> a. $(A^T)^T = A$\n> \n> b. $(A + B)^T = A^T + B^T$\n> \n> c. $(kA)^T = k(A^T)$\n> \n> > [!Warning]\n> > d, e „ÅØÁ∑öÂûã‰ª£Êï∞ÁâπÊúâ„ÅÆÈù¢ÁôΩ„ÅÑÊÄßË≥™\n> \n>  d. $(AB)^T = B^T A^T$\n> \n> e. $(A^r)^T = (A^T)^r \\quad \\text{for all nonnegative integers } r$","x":-510,"y":6123,"width":815,"height":560},
		{"id":"5380508dd61fdeda","type":"text","text":"> [!tip] Theorem 3.5: Properties of Symmetric Matrices\n> a. If $A$ is a square matrix, then $A + A^T$ is a symmetric matrix.\n> \n> b. For any matrix $A$, $AA^T$ and $A^T A$ are symmetric matrices.\n","x":394,"y":6222,"width":535,"height":181},
		{"id":"a777fa6f38d3d9de","x":1294,"y":5404,"width":362,"height":56,"type":"text","text":"## 3.3 The Inverse of a Matrix"},
		{"id":"07a436f13fd511ad","x":1294,"y":5494,"width":508,"height":379,"type":"text","text":"### Inverse\n\n> [!note] Definition: Inverse of a Matrix\n> If $A$ is an $n \\times n$ matrix, an **inverse** of $A$ is an $n \\times n$ matrix $A'$ with the property that:\n> \n> $$\n> AA' = I\n> $$\n> \n> and\n> \n> $$\n> A'A = I\n> $$\n> \n> where $I = I_n$ is the $n \\times n$ identity matrix. If such an $A'$ exists, then $A$ is called **invertible**.\n"},
		{"id":"84d9da5465d43d7f","x":1940,"y":5409,"width":475,"height":170,"type":"text","text":"> [!tip] Theorem 3.6\n> ####  Uniqueness of the Inverse\n> If $A$ is an invertible matrix, then its inverse is unique.\n"},
		{"id":"7162ccdb2f765f6a","x":1940,"y":5605,"width":599,"height":360,"type":"text","text":"> [!tip] Theorem 3.7\n> #### Solution of Linear Systems with an Invertible Matrix\n> If $A$ is an invertible $n \\times n$ matrix, then the system of linear equations given by:\n> \n> $$\n> A\\mathbf{x} = \\mathbf{b}\n> $$\n> \n> has the unique solution:\n> \n> $$\n> \\mathbf{x} = A^{-1}\\mathbf{b}\n> $$\n> \n> for any $\\mathbf{b}$ in $\\mathbb{R}^n$.\n"},
		{"id":"7e0b631421f2762f","x":2620,"y":5605,"width":920,"height":480,"type":"text","text":"> [!cite] Proof\n> ÊñπÁ®ãÂºè $A\\mathbf{x} = \\mathbf{b}$ „Å´Ëß£„Åå„ÅÇ„Çã„Å£„Å¶„Åì„Å®„Å®„ÄÅ„Åù„ÅÆËß£„Åå‰∏Ä„Å§„Å†„Åë„ÇÑ„Å£„Å¶„Åì„Å®„ÇíË®ºÊòé„Åô„Çã„Åß„ÄÇ\n> \n> **1. Ëß£„ÅåÂ≠òÂú®„Åô„Çã„Åì„Å®„ÇíË®ºÊòé„Åô„Çã„ÅßÔºÅ**\n> „Åæ„Åö„ÄÅ$\\mathbf{x} = A^{-1}\\mathbf{b}$ „ÅåÊ≠£„Åó„ÅÑ„Åã„Å©„ÅÜ„ÅãÁ¢∫„Åã„ÇÅ„Çã„Çè„Å™Ôºö\n> \n> $$\n> A\\mathbf{x} = A(A^{-1}\\mathbf{b}) = (AA^{-1})\\mathbf{b} = I\\mathbf{b} = \\mathbf{b}.\n> $$\n> \n> „Åì„Çå„Åß„ÄÅ$\\mathbf{x} = A^{-1}\\mathbf{b}$ „ÅØ„Å°„ÇÉ„Çì„Å® $A\\mathbf{x} = \\mathbf{b}$ „ÇíÊ∫Ä„Åü„Åó„Å¶„Çã„Åã„Çâ„ÄÅËß£„ÅØ**Â∞ë„Å™„Åè„Å®„ÇÇ„Å≤„Å®„Å§„ÅÇ„Çã**„Å£„Å¶„Åì„Å®„ÇÑ„ÄÇ\n> \n> **2. Ëß£„Åå‰∏Ä„Å§„Å†„Åë„ÇÑ„Å£„Å¶„Åì„Å®„ÇíË®ºÊòé„Åô„Çã„ÅßÔºÅ**\n> Ê¨°„Å´„ÄÅ„ÇÇ„ÅóÂà•„ÅÆËß£„Åå„ÅÇ„Çã„Å®‰ªÆÂÆö„Åó„Å¶„ÄÅ„Åù„Çå„Çí $\\mathbf{y}$ „Å®„Åó„Å¶„Åø„Çã„Çè„Å™„ÄÇ„Å§„Åæ„Çä„ÄÅ$A\\mathbf{y} = \\mathbf{b}$ „ÇÑ„Å≠„Çì„ÄÇ„Åª„Çì„Å™„Çâ‰∏°Ëæ∫„Å´ $A^{-1}$ „Çí„Åã„Åë„Çã„ÅßÔºö\n> \n> $$\n> A^{-1}(A\\mathbf{y}) = A^{-1}\\mathbf{b}.\n> $$\n> \n> „Åì„Çå„ÇíÁ∞°Âçò„Å´„Åô„Çã„Å®„Åì„ÅÜ„Å™„ÇãÔºö\n> \n> $$\n> (A^{-1}A)\\mathbf{y} = A^{-1}\\mathbf{b} \\implies I\\mathbf{y} = A^{-1}\\mathbf{b} \\implies \\mathbf{y} = A^{-1}\\\n"},
		{"id":"cea6515eb2eaa635","x":2740,"y":6403,"width":448,"height":252,"type":"text","text":"> [!note] Definition: \n> ### Negative Power of a Matrix\n> If $A$ is an invertible matrix and $n$ is a positive integer, then $A^{-n}$ is defined by:\n> \n> $$\n> A^{-n} = (A^{-1})^n = (A^n)^{-1}.\n> $$\n"},
		{"id":"0625cfabd09b3b41","x":1940,"y":6000,"width":440,"height":383,"type":"text","text":"> [!tip] Theorem 3.8\n> #### Inverse of a 2x2 Matrix\n> If \n> $$\n> A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix},\n> $$\n> then $A$ is invertible if $ad - bc \\neq 0$, in which case:\n> \n> $$\n> A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}.\n> $$\n> \n> If $ad - bc = 0$, then $A$ is not invertible.\n"},
		{"id":"6c78954834f4b9ee","x":1885,"y":6440,"width":710,"height":617,"type":"text","text":"> [!tip] Theorem 3.9:\n> ### Properties of Inverses\n> \n> a. If $A$ is an invertible matrix, then $A^{-1}$ is invertible and:\n> \n> $$\n> (A^{-1})^{-1} = A.\n> $$\n> \n> b. If $A$ is an invertible matrix and $c$ is a nonzero scalar, then $cA$ is an invertible matrix and:\n> \n> $$\n> (cA)^{-1} = \\frac{1}{c} A^{-1}.\n> $$\n> \n> > [!warning] c, d, e  „ÅåÁ∑öÂûã‰ª£Êï∞„Å£„ÅΩ„ÅÑ\n> \n> c. If $A$ and $B$ are invertible matrices of the same size, then $AB$ is invertible and:\n> \n> $$\n> (AB)^{-1} = B^{-1}A^{-1}.\n> $$\n> \n> d. If $A$ is an invertible matrix, then $A^T$ is invertible and:\n> \n> $$\n> (A^T)^{-1} = (A^{-1})^T.\n> $$\n> \n> e. If $A$ is an invertible matrix, then $A^n$ is invertible for all nonnegative integers $n$ and:\n> \n> $$\n> (A^n)^{-1} = (A^{-1})^n.\n> $$\n"},
		{"id":"491977904fc4fe14","x":4800,"y":5569,"width":563,"height":197,"type":"text","text":"> [!tip] Theorem 3.10: Elementary Matrix and Row Operations\n> Let $E$ be the elementary matrix obtained by performing an elementary row operation on $I_n$. If the same elementary row operation is performed on an $n \\times r$ matrix $A$, the result is the same as the matrix $EA$.\n"},
		{"id":"b17bc95648034f23","x":4800,"y":5835,"width":563,"height":130,"type":"text","text":"> [!tip] Theorem 3.11: Invertibility of Elementary Matrices\n> Each elementary matrix is invertible, and its inverse is an elementary matrix of the same type.\n\n"},
		{"id":"92e56ea91c672fee","x":3620,"y":5437,"width":943,"height":873,"type":"text","text":"### Elementary Matrices\n\n> [!note] Definition: Elementary Matrix\n> An **elementary matrix** is any matrix that can be obtained by performing an elementary row operation on an identity matrix.\n\n> [!example] Example: Elementary Matrices and Their Actions\n> Let:\n> \n> $$\n> E_1 = \n> \\begin{bmatrix}\n> 1 & 0 & 0 & 0 \\\\\n> 0 & 3 & 0 & 0 \\\\\n> 0 & 0 & 1 & 0 \\\\\n> 0 & 0 & 0 & 1\n> \\end{bmatrix}, \\quad\n> E_2 = \n> \\begin{bmatrix}\n> 0 & 0 & 1 & 0 \\\\\n> 0 & 1 & 0 & 0 \\\\\n> 1 & 0 & 0 & 0 \\\\\n> 0 & 0 & 0 & 1\n> \\end{bmatrix}, \\quad \\text{and} \\quad\n> E_3 = \n> \\begin{bmatrix}\n> 1 & 0 & 0 & 0 \\\\\n> 0 & 1 & 0 & 0 \\\\\n> 0 & 0 & 1 & 0 \\\\\n> 0 & -2 & 0 & 1\n> \\end{bmatrix}.\n> $$\n> \n> Each of these matrices has been obtained from the identity matrix $I_4$ by applying a single elementary row operation.\n> The matrix $E_1$ corresponds to $3R_2$, \n> $E_2$ to $R_1 \\leftrightarrow R_3$,\n> and $E_3$ to $R_4 - 2R_2$. \n> Observe that when we left-multiply a $4 \\times n$ matrix by one of these elementary matrices, the corresponding elementary row operation is performed on the matrix.\n> \n> For example, if:\n> \n> $$\n> A = \n> \\begin{bmatrix}\n> a_{11} & a_{12} & a_{13} \\\\\n> a_{21} & a_{22} & a_{23} \\\\\n> a_{31} & a_{32} & a_{33} \\\\\n> a_{41} & a_{42} & a_{43}\n> \\end{bmatrix},\n> $$\n> \n> then:\n> \n> $$\n> E_1A = \n> \\begin{bmatrix}\n> a_{11} & a_{12} & a_{13} \\\\\n> 3a_{21} & 3a_{22} & 3a_{23} \\\\\n> a_{31} & a_{32} & a_{33} \\\\\n> a_{41} & a_{42} & a_{43}\n> \\end{bmatrix}, \\quad\n> E_2A = \n> \\begin{bmatrix}\n> a_{31} & a_{32} & a_{33} \\\\\n> a_{21} & a_{22} & a_{23} \\\\\n> a_{11} & a_{12} & a_{13} \\\\\n> a_{41} & a_{42} & a_{43}\n> \\end{bmatrix}, \\quad \\text{and} \\quad\n> E_3A = \n> \\begin{bmatrix}\n> a_{11} & a_{12} & a_{13} \\\\\n> a_{21} & a_{22} & a_{23} \\\\\n> a_{31} & a_{32} & a_{33} \\\\\n> a_{41} - 2a_{21} & a_{42} - 2a_{22} & a_{43} - 2a_{23}\n> \\end{bmatrix}.\n> $$\n"},
		{"id":"69cd7a2fdf06ac53","x":3367,"y":6550,"width":507,"height":199,"type":"text","text":"### Invertibility of Elementary Matrices\n> [!tip] Theorem 3.11: Invertibility of Elementary Matrices\n> Each elementary matrix is invertible, and its inverse is an elementary matrix of the same type.\n"}
	],
	"edges":[
		{"id":"27e3857078cd53b0","fromNode":"8aac3df908ad63c0","fromSide":"right","toNode":"3ff3bf252eab0d04","toSide":"left"},
		{"id":"eafa7d878728a55e","fromNode":"39ae9f27a0b3906d","fromSide":"right","toNode":"8aac3df908ad63c0","toSide":"left"},
		{"id":"3fe2c2cfc343c142","fromNode":"afe4f6e6d1a8c317","fromSide":"right","toNode":"f62f49400e1d88b7","toSide":"left"},
		{"id":"1c62e92101c42f06","fromNode":"2584069adee084f0","fromSide":"right","toNode":"2533422c74c97d56","toSide":"left"},
		{"id":"92d2e09c4ebc986e","fromNode":"2533422c74c97d56","fromSide":"right","toNode":"94a56f5d160f8d6a","toSide":"left"},
		{"id":"ea7086ebf827fdc4","fromNode":"406f7005658e7340","fromSide":"right","toNode":"1b4cd495a61a4ea7","toSide":"left"},
		{"id":"09ae6f58c555e9b9","fromNode":"a663a8355b44526b","fromSide":"right","toNode":"406f7005658e7340","toSide":"left"},
		{"id":"d695f6fb2d00e504","fromNode":"a663a8355b44526b","fromSide":"right","toNode":"20a028c8a7848aaf","toSide":"left"},
		{"id":"3744f8809f6fd32a","fromNode":"a663a8355b44526b","fromSide":"right","toNode":"3d5b418e6fec6764","toSide":"left"},
		{"id":"8194875cc02be835","fromNode":"3d5b418e6fec6764","fromSide":"right","toNode":"8f99bae5dbb985e8","toSide":"left"},
		{"id":"23eef5d7dcd827e1","fromNode":"0b42723ecde3a0fc","fromSide":"right","toNode":"f2faf3e9f5c1ff86","toSide":"left"},
		{"id":"243373b5848e7160","fromNode":"7821604b06949d71","fromSide":"right","toNode":"0f560b2e1a6094fb","toSide":"left"},
		{"id":"f45f9115c4526538","fromNode":"7821604b06949d71","fromSide":"right","toNode":"b7078196d6c5aa74","toSide":"left"},
		{"id":"295f2efe375f071f","fromNode":"63f325337c600605","fromSide":"bottom","toNode":"4c2e13bd5a11f092","toSide":"top"},
		{"id":"24b3b466edc1d597","fromNode":"63f325337c600605","fromSide":"bottom","toNode":"ca65d19aec8bef69","toSide":"top"},
		{"id":"dd5309f86b897141","fromNode":"afe4f6e6d1a8c317","fromSide":"top","toNode":"39ae9f27a0b3906d","toSide":"bottom"},
		{"id":"b6f33a5d109e32d7","fromNode":"2a12d16ef3f7ffb3","fromSide":"right","toNode":"4b85b11f1d1af140","toSide":"left"},
		{"id":"55d61a55125eaab3","fromNode":"2a12d16ef3f7ffb3","fromSide":"right","toNode":"7baa262b93ca3fb0","toSide":"left"},
		{"id":"209451defcd10e96","fromNode":"0d9d8a10a260ce79","fromSide":"right","toNode":"a0e17faa77656cf4","toSide":"left"},
		{"id":"04a62d37e589ed1d","fromNode":"d9278b565480d376","fromSide":"right","toNode":"e02bc77a41174546","toSide":"left"},
		{"id":"156aa11716a7730d","fromNode":"8e941bba141b5a51","fromSide":"right","toNode":"db2670541536258e","toSide":"left"},
		{"id":"980d5e22f051575c","fromNode":"0d9d8a10a260ce79","fromSide":"right","toNode":"a77d8099dc1ff00d","toSide":"top"},
		{"id":"145a46a2d474744a","fromNode":"4090a8b66c9c27f9","fromSide":"bottom","toNode":"7821604b06949d71","toSide":"top"},
		{"id":"f825fc8b23f8094d","fromNode":"4090a8b66c9c27f9","fromSide":"bottom","toNode":"63f325337c600605","toSide":"top"},
		{"id":"0d4fc23206671b3b","fromNode":"8e941bba141b5a51","fromSide":"right","toNode":"d9cccd556dd0287f","toSide":"left"},
		{"id":"05a71e54998fb983","fromNode":"b0fa401e849cc3e0","fromSide":"right","toNode":"5380508dd61fdeda","toSide":"left"},
		{"id":"02ddc146a0900ae4","fromNode":"7162ccdb2f765f6a","fromSide":"right","toNode":"7e0b631421f2762f","toSide":"left"},
		{"id":"f55fb51b635abc2a","fromNode":"07a436f13fd511ad","fromSide":"right","toNode":"84d9da5465d43d7f","toSide":"left"},
		{"id":"ff75dd7638265f46","fromNode":"07a436f13fd511ad","fromSide":"right","toNode":"7162ccdb2f765f6a","toSide":"left"},
		{"id":"bce94b10fefffe83","fromNode":"07a436f13fd511ad","fromSide":"right","toNode":"0625cfabd09b3b41","toSide":"left"},
		{"id":"054c30852a3a42cd","fromNode":"b0fa401e849cc3e0","fromSide":"bottom","toNode":"6c78954834f4b9ee","toSide":"left","toEnd":"none","color":"5","label":"‰ºº„Å¶„ÅÑ„ÇãÊÄßË≥™\nd ÔΩú c\n„ÄÄe ÔΩú d, e"},
		{"id":"d2beb5a0094f5689","fromNode":"0d9d8a10a260ce79","fromSide":"right","toNode":"cea6515eb2eaa635","toSide":"top","color":"4"},
		{"id":"821049067239c210","fromNode":"92e56ea91c672fee","fromSide":"right","toNode":"491977904fc4fe14","toSide":"left"},
		{"id":"3847c2d4f1d57c90","fromNode":"92e56ea91c672fee","fromSide":"right","toNode":"b17bc95648034f23","toSide":"left"},
		{"id":"8babb3267b42e131","fromNode":"07a436f13fd511ad","fromSide":"bottom","toNode":"69cd7a2fdf06ac53","toSide":"left","color":"6"},
		{"id":"c1ba32976073e502","fromNode":"92e56ea91c672fee","fromSide":"bottom","toNode":"69cd7a2fdf06ac53","toSide":"top","color":"6"}
	]
}